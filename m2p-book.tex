\documentclass[10pt]{book}
\usepackage[T1]{fontenc}
\usepackage[margin=1.5in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{tikz-cd}
\usepackage{enumitem}
\usepackage{setspace,microtype}
\usepackage{soul,mdframed}
\usepackage[dvipsnames]{xcolor}
\usepackage{marginnote}
\usepackage{imakeidx}
\usepackage{xifthen}
\usepackage[pdfusetitle,bookmarks,bookmarksnumbered,bookmarksopen,bookmarksopenlevel=1,colorlinks,
citecolor=CornflowerBlue,urlcolor=CornflowerBlue,linkcolor=BurntOrange]{hyperref}
\usepackage[capitalize]{cleveref}
\usepackage[all]{hypcap}

\usepackage[pagestyles]{titlesec}
\titleformat{\chapter}
  {\large\bfseries\filcenter}{Chapter~\thechapter\quad}{1ex}
  {}[]
\titleformat*{\chapter}
  {\large\bfseries\filcenter}
\titleformat*{\section}{\Large}
\titleformat*{\subsection}{\large}

\newpagestyle{main}{
\sethead[\thepage][\textit\chaptertitle][] % even
{}{\thesection\quad\sectiontitle}{\thepage}} % odd
\pagestyle{main}

% \usepackage{titletoc}
% \contentsmargin{2.4em}
% \titlecontents{chapter}
% [2em] 
% {\addvspace{.5\baselineskip}}
% {\contentslabel{2em}}
% {\hspace*{-2em}}
% {\hfill\contentspage}
% \titlecontents{section}
% [4.8em]
% {}
% {\contentslabel{2.8em}}
% {\hspace*{-2.8em}}
% {\titlerule*[1pc]{.}\contentspage}

\makeindex[title=List of Definitions]

% no parentheses
\newtheoremstyle{plain-star}{\topsep}{\topsep}{}{}{\sffamily}{.}{5pt plus 1pt minus 1pt}{\thmnumber{#2 }\thmname{#1}\thmnote{ #3}}

\newtheoremstyle{definition-star}{\topsep}{\topsep}{}{}{\sffamily}{.}{5pt plus 1pt minus 1pt}{\thmnumber{#2 }\thmname{#1}\thmnote{ #3}}

\newtheoremstyle{remark-star}{.5\topsep}{.5\topsep}{}{}{\itshape\sffamily}{.}{5pt plus 1pt minus 1pt}{\thmnumber{#2 }\thmname{#1}\thmnote{ #3}}

\renewcommand\thesection{\thechapter.\Alph{section}}
\numberwithin{equation}{chapter}
\theoremstyle{plain-star}
\newtheorem{thm}[equation]{Theorem}
\newtheorem*{thm*}{Theorem}
\newtheorem{prop}[equation]{Proposition}
% \newtheorem*{prop*}[equation]{Proposition}
\newtheorem{fact}[equation]{Fact}
\newtheorem{lem}[equation]{Lemma}
\newtheorem{cor}[equation]{Corollary}
\theoremstyle{definition-star}
\newtheorem{defn}[equation]{Definition}
\newtheorem{exa}[equation]{Example}
\newtheorem{xca}[equation]{Exercise}
\theoremstyle{remark-star}
\newtheorem{rem}[equation]{Remark}
\newtheorem*{rem*}{Remark}

\newenvironment{sketch}[1][Sketch]{\begin{proof}[#1]\renewcommand*{\qedsymbol}{$\triangle$}}{\end{proof}}

\makeatletter
\newcommand\thmsname{Theorem}
\newcommand\nm@thmtype{theorem}
\theoremstyle{plain-star}
\newtheorem{namedtheorem}[equation]{\thmsname}
\newenvironment{namedthm}[1][Undefined Theorem Name]{
    \ifx{#1}{Undefined Theorem Name}
    \renewcommand\nm@thmtype{theorem}
    \else\renewcommand\thmsname{#1}
    \renewcommand\nm@thmtype{namedtheorem}
    \fi
    \begin{\nm@thmtype}\def\@currentlabelname{#1}}
    {\end{\nm@thmtype}}

\newtheorem*{namedtheorem*}{\thmsname}
\newenvironment{namedthm*}[1][Undefined Theorem Name]{
    \ifx{#1}{Undefined Theorem Name} \renewcommand\nm@thmtype{theorem*}
    \else\renewcommand\thmsname{#1}
    \renewcommand\nm@thmtype{namedtheorem*}
    \fi
    \begin{\nm@thmtype}}
    {\end{\nm@thmtype}}
\makeatother

% \renewcommand{\arraystretch}{1.2}

\setlength{\parskip}{0em} % default parskip
\setlist{listparindent=\parindent,parsep=0pt,left=\parindent} % indentation and separation between list paragraphs
\setlist[enumerate,1]{label=(\alph*)}
\setlist[enumerate,2]{label=(\roman*)}
% \counterwithin*{footnote}{section} %footnote changes based on section
\mdfdefinestyle{simple}{%
    innerleftmargin=0pt,
    innerrightmargin=0pt,
    innertopmargin=2pt,
    innerbottommargin=2pt
}

\makeatletter
% \onehalfspacing
\usepackage{xpatch}
\xpatchcmd{\env@cases}{1.2}{1.1}{}{}
\xpatchcmd{\proof}{\itshape}{\normalfont\proofnamefont}{}{}
\newcommand{\proofnamefont}{\itshape\sffamily}

\let\@subtitle\@empty % default value
\protected\def\subtitle#1{\gdef\@subtitle{#1}}

\renewcommand{\maketitle}{%
    \begin{titlepage}
    \renewcommand\thefootnote{\@fnsymbol\c@footnote}
    \let\footnotesize\small
    \let\footnoterule\relax
    \let \footnote \thanks
    \null\vfil
    \vskip 60\p@
    \begin{center}%
        {\LARGE \@title \par}%
        \vskip .5em%
        {\large \@subtitle \par}%
        \vskip 3em%
        {\large
        \lineskip .75em%
        \begin{tabular}[t]{c}%
            \@author
        \end{tabular}\par}%
        \vskip .75em%
        {\large \@date \par}% % Set date in \large size.
    \end{center}\par
    \@thanks
    \vfil\null
    \end{titlepage}%
    \setcounter{footnote}{0}%
    \global\let\thanks\relax
    \global\let\maketitle\relax
    \global\let\@thanks\@empty
    \global\let\@author\@empty
    \global\let\@date\@empty
    \global\let\@title\@empty
    \global\let\title\relax
    \global\let\author\relax
    \global\let\date\relax
    \global\let\and\relax
}

\newcommand{\R}{\mathbf{R}}
\newcommand{\C}{\mathbf{C}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\N}{\mathbf{N}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\ind}{\mathbf{1}}
\renewcommand{\Pr}{P}
\newcommand{\E}{\mathop{}\!\mathrm{E}}
\newcommand{\Var}{\operatorname{Var}}
\newcommand{\Cov}{\operatorname{Cov}}
\newcommand{\giv}{\,|\,}
\newcommand{\A}{\mathcal{A}}
\newcommand{\B}{\mathcal{B}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}
\newcommand{\M}{\mathcal{M}}
\newcommand{\Mat}{\operatorname{Mat}}
\renewcommand{\Re}{\operatorname{Re}}
\renewcommand{\Im}{\operatorname{Im}}
\newcommand{\ol}[1]{\overline{#1}}
\newcommand{\eR}{\ol{\R}}
\newcommand{\cpl}{\mathrm{c}}
\newcommand{\trp}{\mathrm{T}}
\newcommand{\la}{\langle}
\newcommand{\ra}{\rangle}
\newcommand{\inp}[2]{\langle #1, #2 \rangle}
\newcommand{\nm}[1]{\lVert #1 \rVert}
\newcommand{\abs}[1]{\lvert #1 \rvert}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\blank}{\,\cdot\,}
\renewcommand{\phi}{\varphi}
\newcommand{\tv}[1]{\nm{#1}_{\mathrm{TV}}}
\newcommand{\KL}[2]{D_{\mathrm{KL}}(#1\Vert #2)}
\newcommand{\Ent}{\operatorname{Ent}}
\newcommand{\support}{\operatorname{support}}
\newcommand{\symdiff}{\triangle}
\newcommand{\wkconv}{\Rightarrow}
% \renewcommand{\implies}{\Rightarrow}
% \renewcommand{\impliedby}{\Leftarrow}

% requires xifthen; for automatic/manual indexing
\newcommand{\df}[2][]{\ifthenelse{\isempty{#1}}{\textit{#2}\index{#2}}{\textit{#2}\index{#1}}}

\usepackage[backend=biber,style=alphabetic,doi=false,url=false,isbn=false]{biblatex}
\addbibresource{MP.bib}
\renewbibmacro{in:}{}

% disable automatic page designation
\DeclareFieldFormat{postnote}{#1}


\title{From Measure to Probability}
\subtitle{A survey of measure-theoretic results by a probabilist}
\author{Feng Cheng\thanks{Email: \href{mailto:fecheng@uw.edu}{\texttt{fecheng@math.washington.edu}}. Affiliation: Department of Mathematics, University of Washington, Seattle, WA 98195, USA.}}
\date{Draft as of \today}

\begin{document}
\maketitle

\tableofcontents
\newpage

\phantomsection
\chapter*{\Large Prologue}
\addcontentsline{toc}{part}{Prologue}
\chaptermark{Prologue}

This is the most ambitious writing project undertaken by the author so far as a math student, and he hopes he can finish it in two years. The author, as a probability student, did not excel in his real analysis courses (MATH 202AB at UC Berkeley) during his senior year. To compensate, the author aims to write an extensive and detailed note that surveys through all the major measure theory results of interest to a rigorous-minded mathematical probabilist.

Part I of this note will be devoted to measure theory in a general setting, while Part II will discuss results in probability spaces built on top of Part I. The author hopes that his commentary and the overall structure of the survey can help the readers (and himself) truly understand both abstract measure theory and probability theory from a measure-theoretic point of view.

This entire survey will be based on multiple sources, listed in the bibliography page. As the old saying goes, ``if you copy from one book that is plagiarism, but if you copy from ten books that is scholarship.''
\vspace{1\baselineskip}

\noindent Shanghai, August 2024 \hfill F.C.

\vspace{3\baselineskip}
The prerequisite for this survey notes is a strong background in undergraduate real analysis and familiarity with elementary probability theory. Some key results about Banach spaces, Hilbert spaces, and topology will be assumed, and these can usually be found on any functional analysis texts. We have also included appendices at the end of the survey, which discuss some of these facts.

If you see any errors or typos, please inform the author via \begin{center}
    \href{mailto:fecheng@uw.edu}{\texttt{fecheng@math.washington.edu}}.
\end{center}
\newpage

\part{Measure theory}
\chapter{Measure spaces}
\section{Basic setup}
We let $X$ be a nonempty set in Part I.
\begin{defn}
    For $\{A_n\}_{n=1}^\infty\subseteq \wp(X)$, we define \[
        \limsup_{n \to \infty} A_n = \bigcap_{n=1}^\infty \bigcup_{m=n}^\infty A_m \quad \text{and}\quad \liminf_{n \to \infty} A_n = \bigcup_{n = 1}^\infty \bigcap_{m = n}^\infty A_m.
    \]
\end{defn}
Note $\bigcap$ can be seen as ``for all'' and $\bigcup$ can be seen as ``there exists''. Therefore $\limsup_n A_n$ consists of elements that belong to infinitely many $A_n$'s (spread out across $n \in \N$), while $\liminf_n A_n$ consists of elements that belong to all but finitely $A_n$ (the $n$'s at the beginning). To compare this with the $\limsup$ and $\liminf$ of a sequence of numbers, one may try the following exercise.

\begin{xca}
    Show that \begin{align*}
        \limsup_{n \to \infty}  A_n = A & \iff \limsup_{n \to \infty} \ind_{A_n} = \ind_A, \\
        \liminf_{n \to \infty}  A_n = A & \iff \liminf_{n \to \infty} \ind_{A_n} = \ind_A.
    \end{align*}

    Here $\ind_A\colon X \to \{0,1\}$ given by \[
        \ind_A(x)= \begin{cases}
            1 & \text{if } x \in A, \\
            0 & \text{if } x \not\in A.
        \end{cases}
    \]
    is called the \df{indicator function} (\df[characteristic function (measure theory)]{characteristic function} for analysts who choose to write $\chi_A$).
\end{xca}

If $\{A_n\}_{n=1}^\infty$ is an increasing sequence of sets, then \[
    \liminf_n A_n = \limsup_n A_n = \bigcup_n A_n;
\] if the sequence is decreasing, then \[
    \liminf_n A_n = \limsup_n A_n = \bigcap_n A_n.
\] Also remember that, by De Morgan's Law, \[
    \limsup_n A_n^\cpl = \bigl(\liminf_n A_n\bigr)^\cpl \quad \text{and}\quad 
    \liminf_n A_n^\cpl = \bigl(\limsup_n A_n \bigr)^\cpl.
\]

Here is another exercise.

\begin{xca}
    Consider a sequence of functions $f_n$ that convergences to $f$ pointwise on some set $E$. If we define \[
        E_{n,\epsilon} = \{x:\abs{f_n(x) - f(x)} < \epsilon\}
    \] for $\epsilon > 0$ and $n\in \N$, then \[
        E = \bigcap_{k=1}^\infty \liminf_m E_m^{1/k} = \bigcap_{k=1}^\infty \bigcup_{m=1}^\infty \bigcap_{n\geq m} E_n^{1/k}.
    \]
\end{xca}

\begin{defn}
    A nonempty collection of subsets of $X$ is an \df{algebra} if \begin{enumerate}
        \item $\emptyset,X \in \A$;
        \item closed under complement;
        \item \label{enu:algebra-finite-union} closed under finite unions and intersections.
    \end{enumerate}
    Furthermore, $\A$ is called a \df[sigma-algebra@$\sigma$-algebra]{$\sigma$-algebra} if condition~\ref{enu:algebra-finite-union} asks for countable unions and intersections.
\end{defn}

An algebra can be constructed from a more basic structure called semialgebra, which we define below.
\begin{defn}
    A \df{semialgebra} $\mathcal{E}$ is a collection of sets such that \begin{enumerate}
        \item \label{enu:cond1-semialgebra}$\emptyset \in \mathcal{E}$;
        \item closed under finite intersections;
        \item if $A \in \mathcal{E}$ then $A^\cpl$ is a finite disjoint union of elements in $\mathcal{E}$.
    \end{enumerate}
\end{defn}
Some authors drop condition~\ref{enu:cond1-semialgebra}, while others add the condition that $X \in \mathcal{E}$. But of course there is no essential difference. Now the main result.

\begin{prop}[{\cite[Proposition~1.7]{folland1999}}]
    If $\mathcal{E}$ is a semialgebra\footnote{Folland calls this elementary family.}, then all finite disjoint unions of sets in $\mathcal{E}$ form an algebra.
\end{prop}

The most important example of a semialgebra is the empty set plus all sets of the form \[
    (a_1,b_1] \times \dotsb \times (a_d,b_d] \subseteq \R^d,
\] where $-\infty \leq a_j<b_j \leq \infty$. The finite disjoint unions of half-open half-closed cubes should therefore form an algebra.

From now on we will assume $\A$ is by default a $\sigma$-algebra. Obviously the largest $\sigma$-algebra on $X$ is the power set $\wp(X)$.

Given a $\sigma$-algebra $\A$ on $X$, the couplet $(X,\mathcal{A})$ is called a \df{measurable space}, a space on which we can possibly attach a measure. Given a measurable space $(X,\A)$, we call a set $E$ is $\A$-measurable if $E\in \A$.

Also in analysis, ``$\sigma$'' means countable union while ``$\delta$'' means countable intersection. An \df[F-sigma set@$F_\sigma$ set]{$F_{\sigma}$ set} is a countable union\footnote{\emph{somme} in French} of closed\footnote{\emph{fermé} in French} sets, while a \df[G-delta set@$G_\delta$ set]{$G_\delta$ set} is a countable intersection\footnote{\emph{Durchschnitt} in German} of open\footnote{\emph{Gebiet} in German} sets.

We know that the preimage of a function $f\colon X \to Y$ is a mapping $f^{-1}\colon \wp(Y) \to \wp(X)$ that preserves unions, intersections, and complements, which are also operations in the definition of a $\sigma$-algebra. The next result makes the relationship between the two explicit. See \cref{sec:measurable-functions} for the use.
\begin{prop}[{\cite[Lemma 1.3]{Kallenberg_2002}}] \label{prop:induce-s-alg-meas-map}
    Consider $f\colon X \to Y$, and $\mathcal{M}$ and $\mathcal{N}$ be two respective $\sigma$-algebras on $X$ and $Y$. The preimage $f^{-1}$ induces two $\sigma$-algebras: \begin{enumerate}
        \item \label{enu:backward-ind-s-alg} $\mathcal{M}' =\{f^{-1}(A) : A \in \mathcal{N}\}$ on $X$, in the backward direction; 
        \item \label{enu:forward-ind-s-alg} $\mathcal{N}' = \{B \subseteq Y : f^{-1}(B) \in \mathcal{M}\}$ on $Y$, in the forward direction.
    \end{enumerate}
\end{prop}

\begin{defn}
    Within $X$, given a family of subsets $\mathcal{S}$, the smallest $\sigma$-algebra containing $\mathcal{S}$, i.e., the intersection of all $\sigma$-algebras that contains $\mathcal{S}$, is called the \df[sigma-algebra generated by@$\sigma$-algebra generated by!sets]{$\sigma$-algebra generated by $\mathcal{S}$} , denoted by $\sigma(\mathcal{S})$.

    Similar definitions hold for other types of structures.
\end{defn}
Remember the phrase ``the generated is the smallest'', and this should indicate how some proofs should proceed.

Check that the intersection of a family of algebras/$\sigma$-algebras is an algebra/$\sigma$-algebra. Note that the union is not.

If $X$ is a topological space, then the \df[Borel sigma-algebra@Borel $\sigma$-algebra]{Borel $\sigma$-algebra} on $X$, which we denote by $\B_X$ or $\B(X)$, is the $\sigma$-algebra generated by all open sets. One can of course replace the ``open'' here by ``close''.

If $X = \R$ with the standard Euclidean topology, then $\B(\R)$ is generated  
\begin{itemize}
    \item by open intervals (or closed), 
    \item by left-open right-closed intervals (or the other way around), 
    \item by open rays $\{(a,\infty):a\in\R\}$ (or the other way around),
    \item or by close rays $\{[a,\infty):a\in\R\}$ (or the other way around).
    \item One may replace the endpoints of intervals by rationals as well.
\end{itemize}

The first bullet point boils down the fact that an open set in $\R$ can always be written into the disjoint union of a countable number of open intervals. The proof of this requires us to show that 

\begin{xca}
    Given a set $U$ open in $\R$. The relationship $\sim$ on $U$ given by $x \sim y$ if $[x \land y, x \lor y] \subseteq U$ is an equivalence relation.
\end{xca}
The theorem is of significant importance throughout measure theory, and is key to the construction of Lebesgue measure on the real line that we will see soon. The notations $x \land y$ and $x \lor y$ are shorthand for $\min\{x,y\}$ and $\max\{x,y\}$. We will use this later more often.

\begin{defn}
     A \df{measure} $\mu$ on $(X,\A)$ is a function $\mu\colon\A\to[0,\infty]$ such that \begin{enumerate}
         \item $\mu(\emptyset) = 0$;
         \item \label{enu:sigma-additivity-positive-measure} $\mu$ is \df[sigma-additive@$\sigma$-additive]{$\sigma$-additive}, i.e., let $\{E_n\}_{n=1}^\infty$ be any measurable partition of $E \in \A$, we have  \[
            \mu(E) = \mu\biggl(\bigcup_{n=1}^\infty E_n\biggr) = \sum_{n=1}^\infty \mu(E_n).
         \]
     \end{enumerate}
\end{defn}

For two different rearrangements of the same measurable partition of $E$, $\mu(E)$ should have the same value, because the sum of nonnegative values does not change under reordering. An easy way to see this is to note \[
    \sum_{n=1}^\infty a_n= \sup\biggl\{\sum_{n\in I} a_n:I\text{ is a finite subset of }\N\biggr\}.
\] In fact the RHS above is how we define generalized sums over possibly uncountable indices. Therefore condition~\ref{enu:sigma-additivity-positive-measure} makes sense.

From now on we assume by default that $\mu$ is a measure. The triplet $(X,\A,\mu)$ is called a \df{measure space}.

A measure $\mu$ on $(X,\A)$ is a \df{probability measure}\footnote{Why use this name? Because the probability of the entire sample space should be 1.} if $\mu(X) = 1$; $\mu$ is \df[finite measure]{finite} if $\mu(X) < \infty$; and $\mu$ is \df[sigma-finite measure@$\sigma$-finite measure]{$\sigma$-finite} if $X$ can be written as a countable union of measurable sets $A_n \in \A$, each of which is of finite measure. Note for a $\sigma$-finite measure, we can replace this countable collection of finite-measure sets that make up $X$ by an increasing sequence of finite-measure sets. We may even further assume that the sets are mutually disjoint. These assumption can be handy in some proofs.

It is clear that any probability measure is a finite measure, which is in turn a $\sigma$-finite measure. The probability measure is the essential example of a finite measure, because mostly you can normalize the measure of the whole space to $1$.

A $\sigma$-finite measure means it is a normal kind of measure. The Lebesgue measure that we will rigorously see soon, for example, is $\sigma$-finite. Some major results in measure theory, for example the Fubini--Tonelli theorem (see \cref{sec:prod-integrate}), are only true for $\sigma$-finite measure spaces. A measure that is not $\sigma$-finite is considered, in some sense, a little pathological.

The following facts will come up a couple of times.
\begin{fact} \label{fact:restrict-meausre-original-space}
    Fix some $S \in \A$. The function $\nu\colon \A\to [0,\infty]$ given by $\nu(E) = \mu(E \cap S)$ is still a measure on $(X,\A)$.
\end{fact}
\begin{fact} \label{fact:restrict-meausre-restrict-space}
    Fix $S\in \A$. By intersecting $S$ we can get a sub-$\sigma$-algebra $\A|_S$ on $S$, where \[
        \A|_S = \{E \cap S: E\in\A\}.
    \] Such $(S,\A|_S)$ is called a \df{measurable subspace} of $(X,\A)$. Note that $\mu$ restricted to the $\sigma$-algebra $\A|_S$ is a measure on $\A|_S$. We denoted this restricted measure on $(S,\A|_S)$ by $\mu|_S$, or simply $\mu$ when the context is clear. 
\end{fact}

Below are some important basic properties about measures that are used all the time.
\begin{prop}\label{prop:basic-properties-measure}
    We have the following properties about a measure $\mu$ on $(X,\A)$.
    \begin{enumerate}
        \item monotonicity: for $A,B\in \A$, \[
            A\subseteq B \implies \mu(A)\leq \mu(B);
        \]
        \item inclusion-exclusion: for $A,B \in \A$ with $\mu(A \cap B) < \infty$, we have \[
            \mu(A \cup B) = \mu(A) + \mu(B) - \mu(A\cap B).
        \]
        \item \df[sigma-subadditivity@$\sigma$-subadditivity]{$\sigma$-subadditivity}: for possibly intersecting sets\footnote{Recall in $\sigma$-additivity the sets must be mutually disjoint.} $\{E_n\}_{n=1}^\infty \subseteq \A$, \[
            \mu\biggl(\bigcup_{n=1}^\infty E_n\biggr) \leq \sum_{n=1}^\infty \mu(E_n).
        \]
        \item continuity from below: for a sequence of sets $\{E_n\}_{n=1}^\infty \subseteq \A$ that increases to $E$, we have \[
            \mu(E_n) \uparrow \mu(E).
        \]
        \item continuity from above (when the first set is of finite measure): for a sequence of sets $\{E_n\}_{n=1}^\infty \subseteq \A$ with $\mu(E_1)<\infty$ and $E_n \downarrow E$, we have \[
             \mu(E_n) \downarrow \mu(E).
        \]
    \end{enumerate}
\end{prop}
All these properties above require the famous disjointification trick to prove: we partition the sets in question into pairwise disjoint pieces, and then use $\sigma$-additivity of the measure.

Now we discuss two important examples of measure extremely useful in application\footnote{In this note we will avoid going deep into facts/examples/counterexmaples that are ultimately not very useful in practice. One such ``useless'' example that is often mentioned here is the countable-cocountable measure on an uncountable set. One may also list the collection of all countable and cocountable sets as an example of a $\sigma$-algebra earlier, but we have omitted for the same reason. Some results of greater generality and particular examples add further insight to the subject matter and help our understanding, but in many situations this is not the case.}.

The first one is the \df{counting measure}. Consider the measurable space $\bigl(X,\wp(X)\bigr)$. The function $\mu\colon \wp(X) \to [0,\infty]$ given by $\mu(E) = \abs{E}$ is a measure. Basically it counts how many elements are in each subset of $X$.

The second one is the \df{Dirac measure/point mass}. Given $(X,\A)$ and some $x\in X$, we define the function $\delta_X\colon \A \to \{0,1\}$ given by \[
    \delta_x(A) = \begin{cases}
        1 & \text{if } x \in A, \\
        0 & \text{if } x \not\in A.
    \end{cases}
\]
This is clearly a probability measure. Notice its difference from the indicator function. The point mass $\delta_x(A)$ takes in a set and spits out $1$/$0$, while the corresponding indicator $\ind_A(x)$ takes in a point and spits out $1$/$0$.

We now introduce two additional elementary results about measures, which are simple consequences from \cref{prop:basic-properties-measure}. These two results are important in probability theory (especially the second one), but both are indeed purely measure-theoretic.
\begin{cor}[(Upper and lower semicontinuity of measures)]
    For $\{E_n\}_n \subseteq \A$, we have \[
        \mu\bigl(\liminf_n E_n\bigr) \leq \liminf_n \mu(E_n).
    \]
    If in addition $\mu$ is finite, then \[
        \limsup_n \mu(E_n) \leq \mu\bigl(\limsup_n E_n\bigr).
    \]
\end{cor}

\begin{namedthm}[Borel--Cantelli lemma, part I] \label{thm:BorelCantelli-meas-th}
    For $\{E_n\}_n \subseteq \A$, assume $\sum_{n} \mu(E_n) <\infty$, then \[
        \mu\bigl(\limsup_n E_n\bigr) = 0.
    \]
\end{namedthm}

One can skip the rest of this section for now, and come back after reading about the Lebesgue measure on the real line.

Given $(X,\A,\mu)$, a subset $E\subseteq X$ is called a \df{null set}
if there is $B\in\mathcal{A}$ such that $E\subseteq B$ and $\mu(B)=0$.
If $\A$ contains all these null sets, then the measure space is \df{complete}.
The \df{completion} $\A^\mu$ is the smallest $\sigma$-algebra containing $\A$ such that there exists a measure $\bar{\mu}$, which extends $\mu$ to $\A^\mu$, that makes $(X,\A^\mu)$ complete.

Why is a complete measure space sometimes desirable? We want to make all subsets of measure zero sets measurable to avoid some technical peculiarity. However, even a complete measure space $(X,\A,\mu)$ may still not measure every subset of $X$.

The completion of a measure space is given explicitly, as stated in the following theorem.

\begin{thm}[{\cite[Theorem 1.9]{folland1999}}]
    The completion $\A^\mu$ is unique, which is given by \[
        \A^\mu =\{E\cup F:E\in\A\text{ and }F\subseteq N\text{, where }N\text{ is a null set}\}.
    \]
    In addition, the measure $\bar{\mu}$ given by $\bar{\mu}(E\cup F)=\mu(E)$ not only completes $\A$, but also is the unique extension of $\mu$ from $\A$ to $\A^\mu$.
\end{thm}
\begin{proof}
    The first part of the proof is given in the reference. For the uniqueness part, suppose there is some other measure $\hat{\mu}$ on $\A^\mu$ such that $\hat{\mu}(E) = \mu(E)$ for all $E \in \A$. However, there exists some $D \subseteq N$, where $\mu(N) = 0$, such that $\hat{\mu}(E \cup D) \neq \mu(E) = \hat{\mu}(E)$. This implies $\hat{\mu}(D - E) > 0$. Yet $D - E \subseteq N$ where $\hat{\mu}(N) = 0$. This contradicts monotonicity.
\end{proof}

To similarly avoid peculiarities caused by null sets, we give the following definitions. The set $A \in \A$ is called an \df{atom} of the measure $\mu$ if the set has positive measure $\mu(A)$, but all its measurable subsets must be either of measure $0$ or of measure $\mu(A)$. A measure is \df{atomless} if there are no atoms.

\section{Two tools from set theory}
\begin{defn}
A \df[pi-system@$\pi$-system]{$\pi$-system} on $X$ is a nonempty collection of subsets of $X$ that is closed under finite intersections.

A \df[lambda-system@$\lambda$-system]{$\lambda$-system} $\mathcal{L}$ on $X$ is a collection of subsets of $X$ such that 
\begin{enumerate}
    \item $X\in\mathcal{L}$; 
    \item \label{enu:prop-diff-lam-sys} if $A,B\in\mathcal{L}$ and $A\subseteq B$, then $B-A\in\mathcal{L}$; (closed under proper differences)
    \item if $A_{n}\in\mathcal{L}$ and $A_{n}\uparrow A$ then $A\in\mathcal{L}$. (closed under ascending countable unions)
\end{enumerate}
\end{defn}
\begin{defn}
    A \emph{monotone class} on $X$ is a collection of subsets of $X$ that is closed under ascending countable unions and descending countable intersections.
    
\end{defn}
\begin{namedthm}[Dynkin's $\pi$-$\lambda$ theorem] \label{thm:pi-lambda}
     Within $X$, if $\mathcal{K}$ is a $\pi$-system that is contained in a $\lambda$-system $\mathcal{L}$, then $\sigma(\mathcal{K})\subseteq\mathcal{L}$.
\end{namedthm}
\begin{namedthm}[Monotone class theorem] \label{thm:monotone-class}
     Given an algebra $\A_0$ of sets, then the monotone class $\mathcal{M}$ generated by $\A_0$ coincides with the $\sigma$-algebra $\sigma(\A_0)$ generated by $\A_0$.
\end{namedthm}

We do not prove these results in this note; they are very complicated and not very interesting in the end. ``The generated is the smallest'' is the main idea behind these proofs though. The proof of the next result should provide the readers with a general idea what proofs of this sort look like.

This next result is of theoretical significance. It tells us a $\pi$-system that generates the $\sigma$-algebra identifies the measure. 

Suppose we want to show some property holds on the entire $\A$. The way we apply the \nameref{thm:pi-lambda} usually looks like this. First we prove that the collection of sets with this property is a $\lambda$-system. If we have a $\pi$-system with this property that generates $\A$, then the entire $\A$ must agree with this $\lambda$-system.

\begin{namedthm}[Coincidence criterion {\cite[Proposition~1.15]{Ambrosio_2011}}]
    Let $\mu_1$ and $\mu_2$ be two measures on $(X,\A)$. Suppose we can find a $\pi$-system $\mathcal{K}$ on which the two measures agree, and $\sigma(\mathcal{K}) = \A$.

    If $\mu_1(X) = \mu_2(X) < \infty$ (for example, both are probability measures), then the two measures agree on the entire $\A$.

    More generally, if there exists $\{X_n\} \subseteq \mathcal{K}$ such that $X_n \uparrow X$ and \[
        \mu_1(X_n) = \mu_2(X_n) <\infty \text{ for all }n\in \N,
    \] then the two measures agree on the entire $\A$.
\end{namedthm}
\begin{proof}
    Assume $\mu_1(X) = \mu_2(X) < \infty$. Define $\mathcal{D}$ to be the collection of all sets in $\A$ on which the two measures agree. It is easy to verify that $\mathcal{D}$ becomes a $\lambda$-system. Now invoke \nameref{thm:pi-lambda} and conclude that $\mathcal{D} = \mathcal{A}$. Without the finiteness assumption, we cannot verify condition~\ref{enu:prop-diff-lam-sys} for a $\lambda$-system  that makes $\mu(B) - \mu(A)$ computable.

    Now consider the general assumption. We define for each $n$ \begin{align*}
        \A_n & = \{E \cap X_n : E \in \A\}\text{, which is a $\sigma$-algebra, and} \\
        \mathcal{K}_n & = \{E \cap X_n : E \in \mathcal{K}\}\text{, which is a $\pi$-system contained in $\A_n$.}
    \end{align*}
    Then $\mu_1$ and $\mu_2$ restricted to $(X_n,\A_n)$ is a finite measure. By the special case above, the two measures coincide on $\sigma(\mathcal{K}_n)$.

    Now we prove $\A_n \subseteq \sigma(\mathcal{K}_n)$. Check that since $X_n \in \mathcal{K}$, \[
        \{E \subseteq X : E \cap X_n \in \sigma(\mathcal{K}_n)\}
    \] is a $\sigma$-algebra containing $\mathcal{K}$, and hence $\A$.

    Now for each $n$ and all $E \in \mathcal{A}$, the two measures agree on $E \cap X_n$. Now take $n \to \infty$ and we see that $\mu_1 = \mu_2$.
\end{proof}

\section{Extension theorems}
\begin{defn}
    The \df{outer measure} on $X$ is a function $\mu^{*}\colon\wp(X)\to[0,\infty]$ such that 
    \begin{enumerate}
    \item $\mu^{*}(\emptyset)=0$; (emptyset)
    \item if $A\subseteq B$, then $\mu^{*}(A)\leq\mu^{*}(B)$; (monotonicity)
    \item For subsets $A_{1},A_{2},\dotsc$ of $X$, $\mu^{*}(\cup_{i=1}^{\infty}A_{i})\leq\sum_{i=1}^{\infty}\mu^{*}(A_{i})$. ($\sigma$-subadditivity)
    \end{enumerate}
    A \df[outer null set]{null set} with respect to the outer measure $\mu^{*}$ is just a set with $\mu^{*}$-value 0.
\end{defn}

Let $\mathcal{C}$ be a collection of subsets of $X$ such that $\emptyset\in\mathcal{C}$ and there are $D_{1},D_{2},\dotsc$ in $\mathcal{C}$ such that $\cup_{i\in\N}D_{i}=X$. Suppose $\ell\colon\mathcal{C}\to[0,\infty]$ with $\ell(\emptyset)=0$. Now if we define for all $E\in \wp(X)$ 
\[
    \mu^{*}(E)=\inf\biggl\{\sum_{i=1}^{\infty}\ell(A_{i}):E\subseteq\bigcup_{i=1}^{\infty}A_{i}\text{, where every }A_{i}\in\mathcal{C}\biggr\},
\]
then $\mu^{*}$ is an outer measure on $X$. (Note that by assumption
the infimum is taken over a nonempty set, and hence always exists. For simplicity one may just assume $X \in \mathcal{C}$ as well.)
The proof is routine.

    Here are some forewords to what we will construct.
\begin{itemize}
\item Let $X=\R$, $\mathcal{C}$ be the collection of all left-open right-closed
intervals, and $\ell\bigl((a,b]\bigr)=b-a$. This gives the Lebesgue
outer measure $m^{*}$ used to construct the Lebesgue measure $m$.
\item Let $f\colon\R\to\R$ be an increasing right-continuous\footnote{We will use ``increasing'' and ``strictly increasing'' in our note. Right-continuity at $x$ means continuity from $x^{+}$.}
function. we let $\ell\bigl((a,b]\bigr)=f(b)-f(a)$. The $\mu^{*}$
that arises from this is used to construct the Lebesgue--Stieltjes measure.
\end{itemize}

\begin{defn}
    For an outer measure $\mu^{*}$, a set $A\subseteq X$ is \df[outer measurable]{$\mu^{*}$-measurable} if for all $E\subseteq X$, \[
        \mu^{*}(E)=\mu^{*}(E\cap A)+\mu^{*}(E\cap A^\cpl).
    \] 
\end{defn}

    This characterizes a collection of sets that are well-behaved under set operations, which leads to the next theorem. Note it $A$ is $\mu^{*}$-measurable if and only if for all $E$ with $\mu^{*}(E)<\infty$, \[
        \mu^{*}(E)\geq\mu^{*}(E\cap A)+\mu^{*}(E\cap A^\cpl).\]
\begin{namedthm}[Carathéodory's theorem]
    Given an outer measure $\mu^{*}$ on $X$, then the collection $\A$ of $\mu^{*}$-measurable sets is in fact a $\sigma$-algebra on $X$. Let $\mu=\mu^{*}|_{\mathcal{A}}$, then $\mu$ is a measure. Also the $\sigma$-algebra $\A$ contains all the null sets, i.e., $(X,\A,\mu)$ is complete.
\end{namedthm}

\begin{proof}
$\A$ is clearly closed under complements. We then check $\A$ is an algebra (the union of two sets in $\A$ is still in $\A$), and show $\mu^{*}$ is finitely additive on $\A$.

We wish to extend finite additivity to countable additivity. We let
$B_{n}=\cup_{j=1}^{n}A_{j}$ and $B=\cup_{j=1}^{\infty}A_{j}$. For
any $E$, we may conclude that 
\[
\mu^{*}(E\cap B_{n})=\sum_{j=1}^{n}\mu(E\cap A_{j}).
\]
It follows that $\mu^{*}(E)\geq\sum_{j=1}^{n}\mu^{*}(E\cap A_{j})+\mu(E\cap B^{\mathrm{c}}).$
Take $n\to\infty$ we may conclude 
\begin{align*}
\mu^{*}(E) & \geq\sum_{j=1}^{\infty}\mu^{*}(E\cap A_{j})+\mu^{*}(E\cap B^{\mathrm{c}})\\
 & \geq\mu^{*}\Bigl(\bigcup_{j=1}^{\infty}(E\cap A_{j})\Bigr)+\mu^{*}(E\cap B^{\mathrm{c}})\\
 & =\mu^{*}(E\cap B)+\mu^{*}(E\cap B^{\mathrm{c}})\geq\mu^{*}(E).
\end{align*}
It follows that $B\in\A$, and if we let $E=B$, the first inequality
(which is an equality) gives countable additivity.

It is easy to show $\A$ contains all $\mu^{*}$-null sets: for $N$
such that $\mu^{*}(N)=0$, for any $E$ we have 
\[
\mu^{*}(E)\leq\mu^{*}(E\cap N)+\mu^{*}(E\cap N^{\mathrm{c}})\leq\mu^{*}(E\cap N^{\mathrm{c}})\leq\mu(E).\qedhere
\]
\end{proof}

\begin{namedthm}[Carathéodory's extension theorem] \label{thm:Caratheodory-ext}
    For algebra $\A_{0}$ on $X$ and its premeasure
$\mu_{0}$, let 
\[
\mu^{*}(E)=\inf\biggl\{\sum_{i=1}^{\infty}\mu_{0}(A_{i}):E\subseteq\bigcup_{i=1}^{\infty}A_{i}\text{, where every }A_{i}\in\A_{0}\biggr\}
\]
for all $E\subseteq X$. Then (1) $\mu^{*}$ is an outer measure on $X$, and hence by Carathéodory's theorem it gives a meausure space
$(X,\sigma(\A_{0}),\mu)$; (2) $\mu^{*}|_{\A_{0}}=\mu_{0}$; (3) every
set in $\A_{0}$ is $\mu^{*}$-measurable; (4) if $\mu_{0}$ is $\sigma$-finite,
then $\mu$ in (1) is the unique extension of $\mu_{0}$ from $\A_{0}$
to $\sigma(\A_{0})$.
\end{namedthm}

\begin{proof}
When proving $\mu^{*}(E)\geq\mu_{0}(E)$ in (2), consider the disjoint
sets $B_{n}=E\cap(A_{n}-\cup_{i=1}^{n-1}A_{i})$. Then $\cup_{n=1}^{\infty}B_{n}=E$,
which implies $\sum_{n=1}^{\infty}\mu_{0}(A_{n})\geq\sum_{n=1}^{\infty}\mu_{0}(B_{n})=\mu_{0}(E)$.
Then take infimum. (3) is fairly straightforward from definition.

To prove (4), let measure $\nu$ be another extension. Consider $E\in\sigma(\A_{0})$
and $\{A_{i}\}_{i=1}^{\infty}\subseteq\A_{0}$ that covers $E$, we
have 
\[
\nu(E)\leq\sum_{i=1}^{\infty}\nu(A_{i})=\sum_{i=1}^{\infty}\mu_{0}(A_{i}).
\]
Take infimum and we get $\nu(E)\leq\mu(E)$.

Now let $A=\cup_{i=1}^{\infty}A_{i}$, then 
\[
\mu(A)=\lim_{n\to\infty}\mu(\cup_{i=1}^{n}A_{i})=\lim_{n\to\infty}\nu(\cup_{i=1}^{n}A_{i})=\nu(A).
\]
If $\mu(E)<\infty$, then for any $\epsilon>0$ we may choose $\{A_{i}\}_{i=1}^{\infty}$
such that $\mu(A-E)<\epsilon$. It follows that 
\[
\mu(E)\leq\mu(A)=\nu(A)=\nu(E)+\nu(A-E)<\nu(E)+\epsilon.
\]
 Therefore $\mu(E)=\nu(E)$.

Now suppose we have $X=\cup_{j=1}^{\infty}B_{j}$ such that $\mu_{0}(B_{j})<\infty$
and that the $B_{j}$'s are pairwise disjoint. Then for $E\in\sigma(\A_{0})$,
we have 
\[
\mu(E)=\sum_{j=1}^{\infty}\mu(E\cap B_{j})=\sum_{j=1}^{\infty}\nu(E\cap B_{j})=\nu(E),
\]
where the second equality follows from what we have previously.
\end{proof}

\section{The Lebesgue measure}
\begin{fact}
    Assuming the axiom of choice, we can use Zorn's lemma to prove $\mathcal{L} \neq \wp(X)$.
\end{fact}

\begin{thm}
    Lebesgue--Stieltjes measures can be approximated inside by compact sets (in particular, closed sets) and outside by open sets.
\end{thm}

\begin{thm}
    For a finite measure $\mu$ on a metric space $X$ with the Borel $\sigma$-algebra, $\mu$ can be approximated inside by closed sets and outside by open sets.
\end{thm}

Bill Theorem 12.3


This theorem is of particular importance in probability theory.

Every finite measure on a topological space is outer regular if and only if it is inner regular, so compactness would be sufficient

Parthasarathy shows that every finite Borel measure on a metric space is regular (p.27), and every finite Borel measure on a complete separable metric space, or on any Borel subset thereof, is tight (p.29).

\begin{thm}[{\cite[Theorem~1.16]{folland1999}}] \label{thm:increasing-rcont-Borel-measure-connection} \leavevmode
    \begin{enumerate}
        \item \label{enu:CDF-measure} Let $F\colon \R \to \R$ be an increasing, right-continuous function, then there is a unique associated Borel measure $\mu_F$ on $\R$ such that \[
        \mu_F(a,b] = F(b) - F(a)\quad \text{for all }a,b\in \R.
    \] If $G$ is another increasing, right-continuous function, then $\mu_F = \mu_G$ if and only if $F$ and $G$ differ by a constant.
        \item \label{enu:measure-CDF} Conversely, if $\mu$ is a finite Borel measure on $\R$, then the function $F\colon \R \to \R$ given by $F(x) = \mu(-\infty,x]$ is increasing and right-continuous. Furthermore $\mu = \mu_F$, and the function has left limit, i.e., $\lim_{y \to x^-} F(y)$ exists at every $x \in \R$. More specifically, \begin{equation} \label{eq:CDF-left-limits}
             \lim_{y \to x^-} F(y) = \mu(-\infty,x).
        \end{equation}
    \end{enumerate}
\end{thm}

Regarding equation \eqref{eq:CDF-left-limits}, it is customary to write $F(x-) = \lim_{y \to x^-} F(y)$ when the limit exists. Note that having left limits implies \[\mu\{x\} = F(x) - F(x-)\] for all $x \in \R$.

\begin{proof}
    Caratheodory, pi-lambda
\end{proof}

    For part~\ref{enu:measure-CDF}, more generally, if $\mu$ is a Borel measure on $\R$ that is finite on all bounded Borel sets, then $F$ can be instead defined by \[
        F(x) = \begin{cases*}
            \mu (0,x] & if $x > 0$, \\
            0 & if $x = 0$, \\
            -\mu(x,0] & if $x < 0$,
        \end{cases*}
    \] and the same conclusions still hold.

\chapter{Measurable functions and integration}
\section{Measurable functions}\label{sec:measurable-functions}

\begin{defn}
    Given two measurable spaces $(X,\mathcal{M})$ and $(Y,\mathcal{N})$, a function $f\colon X \to Y$ is called a \df{measurable function} if $f^{-1}(A) \in \M$ for all $A \in \mathcal{N}$.
    
    We would stress that the function is $\M/\mathcal{N}$-measurable if the context is not clear. When $(Y,\mathcal{N}) = (\R,\B)$, we usually say $f$ is $\M$-measurable\footnote{Now be aware that either a set or a function may be called $\M$-measurable.}. Therefore when $\M=\B_X$ or $\mathcal{L}_X$, $f$ would be called Borel or Lebesgue measurable, respectively.
\end{defn}

Check on your own that compositions of measurable functions is measurable.

To check measurability, it suffices to just check preimage condition for a collection of subsets that generates the image $\sigma$-algebra $\mathcal{N}$. This is the content of the next proposition, and is a direct consequence of \cref{prop:induce-s-alg-meas-map}\ref{enu:forward-ind-s-alg}.
\begin{prop}\label{prop:measurability-generate}
    If $\mathcal{N}$ is generated by $\mathcal{E}$, then $f\colon X \to Y$ is $\M/\mathcal{N}$-measurable if and only if $f^{-1}(E) \in \M$ for all $E \in \mathcal{E}$.    
\end{prop}

With this sufficient condition in mind, it is easy to check that continuous functions between topological spaces are measurable, and increasing/decreasing functions from $\R$ to $\R$ are Borel-measurable.

Given a set $X$, a measurable space $(Y,\mathcal{N})$, and a function $f\colon X \to Y$, then by \cref{prop:induce-s-alg-meas-map}\ref{enu:forward-ind-s-alg} we know \[
    \{f^{-1}(A):A\in \mathcal{N}\}
\] is the smallest $\sigma$-algebra on $X$ that makes $f$ measurable. We call it the \df[sigma-algebra generated by@$\sigma$-algebra generated by!a function]{$\sigma$-algebra generated by $f$}, denoted by $\sigma(f)$.

More generally, consider a collection of measurable spaces $(Y_\alpha,\mathcal{N}_\alpha)$ over all $\alpha \in I$. Suppose we are given $f_\alpha\colon X \to Y_\alpha$ for all $\alpha$. The \df[sigma-algebra generated by@$\sigma$-algebra generated by!functions]{$\sigma$-algebra generated by the class of functions $\{f_\alpha\}_{\alpha \in I}$} on $X$ is defined to be 
\[
    \sigma(\{f_\alpha\}_{\alpha\in I}) = \sigma\bigl(\cup_{\alpha \in I} \{f^{-1}(A_\alpha):A_\alpha\in \mathcal{N}_\alpha\}\bigr).
\] (Recall that union of $\sigma$-algebras is not necessarily a $\sigma$-algebra.)

\begin{namedthm}[Simple function approximation]
    Given $f \in L^+(X,\A)$, there exists a sequence of nonnegative simple functions $\{s_n\}_{n=1}^\infty$ such that $s_n \uparrow f$ pointwise. Furthermore $s_n \to f$ uniformly on any set on which $f$ is bounded.
\end{namedthm}

Note that the ``furthermore'' part essentially means that every nonnegative bounded measurable function is the increasing uniform limit of nonnegative simple functions.

Folland Ex 2.9

\section{Nonnegative Lebesgue integrals}

Repartition function is cadlag


\begin{namedthm}[Monotone convergence theorem]
    If $\{f_n\} \subseteq L^+$ such that $f_n \uparrow f$ , then \[
        \int f = \lim_n \int f_n
    \]
\end{namedthm}
% Note $f_n \uparrow f$ implies $f = \sup_n f_n$ and is thus measurable. Hence $\int f$ makes sense.
\begin{namedthm}[Fatou's lemma]
    Let $\{f_n\}\subseteq L^+$, then \[
        \int \bigl(\liminf_n f_n\bigr) \leq \liminf_n \int f_n
    \]
\end{namedthm}

\section{Signed Lebesgue integrals}
\begin{namedthm}[Lebesgue dominated convergence theorem]
    Let $\{f_n\}\subseteq L^1$. If 
    \begin{enumerate}
        \item $f_n \to f$ pointwise a.e., [limit]
        \item and there exists some nonnegative $g \in L^1$ such that $\abs{f_n} \leq g$ a.e.\ for all $n$, [bound]
    \end{enumerate}
    then $f \in L^1$ with the $L^1$ convergence \[
        \lim_n \int \abs{f - f_n} = 0.
    \] In particular, we have \[
        \int f = \lim_n \int f_n.
    \]
    
\end{namedthm}

This next result is a special case of the above one.
\begin{namedthm}[Bounded convergence theorem] \label{thm:bdd-conv-thm}
    Say $\mu(X) < \infty$. Let $\{f_n\} \subseteq L^1$. If \begin{enumerate}
        \item $f_n \to f$ pointwise a.e.,
        \item and there exists some $M \in \R^+$ such that $\abs{f_n} \leq M$ a.e.\ for all $n$, 
    \end{enumerate}
    then $f \in L^1$ with the $L^1$ convergence \[
        \lim_n \int \abs{f - f_n} = 0.
    \] In particular, we have \[
        \int f = \lim_n \int f_n.
    \]
\end{namedthm}

\begin{namedthm}[Markov's inequality] \label{thm:Markov-ms}
    Let $f\colon X \to \R$ be measurable and $\phi\colon \R \to [0,\infty)$ be increasing (and hence measurable). Then for any $a\in \R$ with $\phi(a)\neq 0$, we have \[
        \mu\{x:f(x) \geq a\} \leq \frac{1}{\phi(a)}\int \phi\circ f\,d\mu.
    \]

    The above statement still holds if we replace all $\R$ above by $[0,\infty)$.
\end{namedthm}
\begin{proof}
    Fix $a$ with $\phi(a)\neq 0$. Using $\phi$ is increasing and nonnegative, we have \begin{align*}
        \phi(a)\mu\{x:f(x) \geq a\} & \leq \int_{\{x:f(x) \geq a\}} \phi(a)\,d\mu(x) \\
        & \leq \int_{\{x:f(x) \geq a\}} \phi\bigl(f(x)\bigr)\,d\mu(x) \\
        & \leq \int \phi\bigl(f(x)\bigr)\,d\mu(x). \qedhere
    \end{align*}
\end{proof}

If we let $\phi(y) = y^p$ ($0<p<\infty$), and use $\abs f$ in place of $f\colon X\to \R$, then we get for any $a > 0$, \[
    \mu\{x:\abs{f}\geq a\} \leq \frac{1}{a^p}\int \abs{f}^p\,d\mu.
\]


\begin{namedthm}[Jensen's inequality] \label{thm:Jensen-ms}
    Let $\mu$ be a probability measure, and $f\in L^1$. Suppose $I$ is an interval containing the range of $f$, and we have a convex function $\phi\colon I\to \R$ such that $\phi\circ f\in L^1$. Then \begin{equation}
        \phi\biggl(\int f\,d\mu\biggr) \leq \int \phi\circ f \,d\mu. \label{eq:jensen}
    \end{equation}
\end{namedthm}

In particular, if $\phi$ is bounded below, then we can drop the integrability assumption on $\phi \circ f$. If \[\int \abs{\phi \circ f}= \int_{\{\phi \circ f \geq 0\}} \phi\circ f + \int_{\{\phi \circ f < 0\}} -\phi\circ f= \infty\] and $\phi$ is bounded below, then \[\int \phi \circ f\,d\mu = \int_{\{\phi \circ f \geq 0\}} \phi\circ f - \int_{\{\phi \circ f < 0\}} -\phi\circ f = \infty.\] Hence the inequality \eqref{eq:jensen} trivially holds.

\section{Connections to the Riemann theory}

\section{Modes of convergence}
\begin{defn}
    For a sequence of measurable functions ${f_n}$, we say $f_n$ converges to some function $f$ 
    \begin{itemize}
        \item \df[convergence!almost everywhere]{almost everywhere} (a.e.) if \[
            \mu\{x: \lim_n f_n(x) = f(x)\}^\cpl = 0.
        \]
        \item \df[convergence!in L-p@in $L^p$]{in $L^p$} ($1 \leq p <\infty$), if $\int \abs{f_n}^p <\infty$ for all $n$, and \[
            \int \abs{f_n - f}^p \to 0.
        \]
        In \cref{sec:Lp-1-infty} we will show that the limiting function $f$ also has $\int \abs{f}^p < \infty$.
        \item \df[convergence!in measure]{in measure} if for any $\epsilon > 0$, \begin{equation} \label{eq:in-measure}
            \lim_n \mu\{x : \abs{f_n(x) - f(x)} > \epsilon\} = 0.
        \end{equation}
    \end{itemize}
    We say $\{f_n\}$ is 
    \begin{itemize}
        \item \df{Cauchy/fundamental in measure} if for any $\epsilon > 0$, there exists $N \in \N$ such that for all $m > n \geq N$, \begin{equation} \label{eq:Cauchy-in-measure}
            \mu\{x:\abs{f_n(x) - f_m(x)} > \epsilon \} < \epsilon
        \end{equation}
    \end{itemize}
    Note that the ``$>$'' in both \eqref{eq:in-measure} and \eqref{eq:Cauchy-in-measure} can be replaced by ``$\geq$'', obviously.
\end{defn}

\begin{thm}[(relationships between different convergences)] \leavevmode
    \begin{enumerate}
        \item all limits above is unique a.e.
        \item $f_n \to f$ in measure implies $\{f_n\}$ is Cauchy in measure.
        \item Convergence in $L^p$ implies convergence in measure.
        \item $f_n \to f$ in measure implies there exists a subsequence $\{f_{n_k}\}$ that converges a.e.\ to $f$ as $k \to \infty$.
        \item If the measure space is finite, then convergence a.e.\ implies convergence in measure.
        \item further subsequence
    \end{enumerate}
\end{thm}
\begin{proof} \leavevmode
    \begin{enumerate} 
        \item 
    \end{enumerate}
\end{proof}

\begin{exa}
MCT Fatou DCT with only convergence in measure    
\end{exa}

\section{Littlewood's second and third principles} \label{sec:Littlewood-2nd-3rd}
\begin{namedthm}[Egoroff's theorem] \label{thm:Egoroff}
    Say $\mu(X)<\infty$. Let ${f_n}$ be a sequence of $\A$-measurable functions from $X$ to $\R$ (or $\C$). Then for all $\epsilon > 0$, there exists some measurable set $E$ such that \[
        \mu(E^\cpl) < \epsilon, \quad \text{while } f_n \to f \text{ uniformly on } E.
    \]
    We call this conclusion $f_n$ converges to $f$ \df[convergence!almost uniformly]{almost uniformly}.
\end{namedthm}

% \begin{xca}[\cite{[}{]}{folland1999}]
%     If $f_n \to f$ almost uniformly, then $f_n \to f$ a.e.\ and in measure.
% \end{xca}
We mention that it is a good exercise to prove \nameref{thm:bdd-conv-thm} using this result.
\begin{namedthm}[Luzin's theorem]
    
\end{namedthm}

\section{Uniformly integrable functions}

Use the material we have discussed so far to prove the following result.
\begin{xca}[\cite{Royden_2023}] \label{xca:abs-cont-int-motiv}
Let $f \in L^1(\mu)$. Then
\begin{enumerate}
    \item \label{enu:abs-cont-int}for all $\epsilon > 0$, there is a $\delta > 0$ such that \[
    \mu(E) < \delta \implies \int_E \abs{f} \,d\mu < \epsilon;
\]
    \item moreover, for each $\epsilon > 0$, there is some $X_0$ with $\mu(X_0) < \infty$ such that \[
    \int_{X - X_0} \abs{f} < \epsilon.
\]
\end{enumerate}
\end{xca}

Notice that \[
    \biggl\vert\int_E f \,d\mu \biggr\vert \leq \int_E \abs{f} \,d\mu = \biggl\vert\int_{E \cap \{f \geq 0\}} f\,d\mu\biggr\vert + \biggl\vert\int_{E \cap \{f < 0\}} -f \,d\mu\biggr\vert.\]
Hence conclusion~\ref{enu:abs-cont-int} is equivalent to $\forall\,\epsilon > 0$, $\exists\, \delta > 0$ such that \[
    \mu(E) < \delta \implies \biggl\vert\int_E f \,d\mu\biggr\vert <\epsilon.
\]

This motivates the next definition, which requires \ref{enu:abs-cont-int} to hold uniformly for a class of integrable functions.

\begin{defn}
    A set of functions $\F \subseteq L^1(\mu)$ has \df{uniformly absolutely continuous integrals} if for every $\epsilon > 0$, there exists $\delta > 0$ such that \[
        \mu(E) < \delta \implies \int_E \abs{f} \,d\mu <\epsilon \text{ for all $f \in \F$},
    \] or equivalently, \[ \biggl\vert\int_E f \,d\mu\biggr\vert <\epsilon \text{ for all $f \in \F$}.
    \]
\end{defn}

The term ``absolutely continuous'' that appear in the definition above is related the notion of an absolutely continuous pair of measures we will discuss in \cref{sec:signed}. Since for $f \in L^1(X,\A,\mu)$, $\nu(E) = \int_E \abs{f} \,d\mu$ defines a finite positive measure $\nu$ on $\A$ that is absolutely continuous with respect to $\mu$. This immediately proves conclusion~\ref{enu:abs-cont-int} in \cref{xca:abs-cont-int-motiv}.

\begin{defn}
    A set of functions $\F\subseteq L^1(\mu)$ is \df{uniformly integrable} if \[
        \lim_{C \to \infty} \sup_{f\in \F} \int_{\{f > C\}} \abs{f}\,d\mu = 0.
    \]
\end{defn}

These two definitions are quite obviously related, as stated by the next proposition.

\begin{prop}
    
\end{prop}

\begin{xca}
    Suppose there exists some $p > 1$ such that $\sup_{n\in \N} \int \abs{f_n}\,d\mu <\infty$, then the collection $\{f_n\}_n$ is uniformly integrable.
\end{xca}

\begin{namedthm}[Vitali's convergence theorem]
    Suppose $\mu$ is finite. Let $\{f_n\} \subseteq L^1(X,\A,\mu)$, then the following are equivalent: 
    \begin{enumerate}
        \item $f \in L^1$ with $f_n \to f$ in $L^1$.
        \item $f_n \to f$ in measure, and $\{f_n\}$ is uniformly integrable.
    \end{enumerate}
\end{namedthm}
\section{Continuity and differentiability of parametrized functions}

\section{Image measures} \label{sec:image-measure}
Consider a measure space $(X,\mathcal{M},\mu)$ and a measurable space $(Y,\mathcal{N})$. If we have an $(\mathcal{M},\mathcal{N})$-measurable function $\phi\colon X \to Y$, then we can define a function $\mu_{*}\colon \mathcal{N} \to [0,\infty]$ given by \[
    \mu_{*}(E) =  \mu(\phi^{-1}E)
\] for all $E\in \mathcal{N}$. This turns out to a measure on $(Y,\mathcal N)$, and we call this the \df{image/pushforward measure} of $\mu$ by $\phi$, denoted by $\phi_*\mu$ or $\phi_{\#}\mu$.

Image measure characterizes change of variables, which is of basic importance in mathematics. We will use image measures later in \cref{sec:cov,sec:polar,sec:expec}.

We state the main result below.
\begin{prop} \label{prop:image-meas-cov}
    Under the conditions stated above, let $g\in L^+(Y,\mathcal{N})$ or $g \circ \phi \in L^1(X,\mathcal{M},\mu)$. Then \begin{equation*}
        \int_X g\bigl(\phi(x)\bigr) \,d\mu(x) = \int_Y g(y) \,d\mu_*(y). %\label{eq:image-m}
    \end{equation*}
\end{prop}
\begin{proof}
    When $g = \ind_E$ for $E \in \mathcal{N}$, we have \[
        \text{LHS} = \mu\{x : \phi(x) \in E\} = \mu(\phi^{-1} E) \quad \text{and} \quad 
        \text{RHS} = \mu_{*}(E).
    \] Now extend this to simple functions, then nonnegative functions, and then integrable functions.
\end{proof}

\chapter{Product spaces}
\section{Product \texorpdfstring{$\sigma$-algebras}{sigma-algebra}}

We start with a comparison between product topologies and product $\sigma$-algebras. See \cite[Sections 4.1 and 4.2]{folland1999} for a review of bases, subbases and product topologies.

For topological spaces $(X_\alpha,\mathcal{T}_\alpha)$ ($\alpha \in I$), recall that the \df{product topology} $\mathcal{T}$ on $X = \prod_{\alpha \in I} X_\alpha$ is the topology generated by all coordinate projections $\pi_\alpha\colon X \to X_\alpha$ (i.e., the smallest topology on $X$ that makes all these maps continuous). Explicitly $\mathcal T$ is generated by the collection of subbasic sets \begin{equation} \label{eq:1d-cylinder-top}
    \{\pi_\alpha^{-1}(U_\alpha): U_\alpha\in \mathcal{T}_\alpha, \alpha \in I\}.
\end{equation}
    
For measurable spaces $(X_\alpha, \A_\alpha)$ ($\alpha \in I$), the \df[product sigma-algebra@product $\sigma$-algebra]{product $\sigma$-algebra} $\A = \bigotimes_{\alpha\in I} \A_\alpha$ on $X = \prod_{\alpha \in I} X_\alpha$ is the $\sigma$-algebra generated by all coordinate projections $\pi_\alpha$. Explicitly $\A$ is generated by the collection of sets \begin{equation} \label{eq:1d-cylinder-sa}
     \{\pi_\alpha^{-1}(E_\alpha): E_\alpha\in \A_\alpha, \alpha \in I\}.
\end{equation}

Define the general \df{cylinder sets}\footnote{This definition similarly holds for other set-collection pairs.} on the product of topological spaces $(X_\alpha,\mathcal{T}_\alpha)$ and measurable spaces $(X_\alpha,\A_\alpha)$ to be the sets of form \[
    \bigcap_{j=1}^n \pi_{\alpha_{j}}^{-1}(U_{\alpha_j}) \quad \text{and} \quad \bigcap_{j=1}^n \pi_{\alpha_{j}}^{-1}(E_{\alpha_j}),
\] for any $n \in \N$, respectively. To put them into simple words, they are finite intersections of preimages of the projections. The collection of sets in \eqref{eq:1d-cylinder-top} and \eqref{eq:1d-cylinder-sa} are $1$-dimensional cylinders.

The general cylinder sets on the product of topological spaces, as finite\footnote{As another reminder, if the intersection is allowed to be arbitrary, then we get a larger topology called the \df{box topology}. The box topology is generated by full-dimension products of open sets. When the product is finite, the box topology and the product topology coincide.} intersections of subbasic sets in \eqref{eq:1d-cylinder-top}, form a basis for the product topology $\mathcal{T}$. However, it is a well-known fact that $\sigma$-algebras, unlike topologies, cannot be written out explicitly from the elementary sets they are generated from. % Therefore in studying product $\sigma$-algebras, we should work primarily with $1$-dimensional cylinder sets of the form \[
%     E_\beta \times \prod_{\alpha \neq \beta} X_\alpha
% \] over all $\beta \in I$ and $E_\beta \in \A_\beta$.

Looking back at \eqref{eq:1d-cylinder-sa}, you may expect a smaller collection of cylinder sets generates the product $\sigma$-algebra. Yet the proof is a little weird, like most arguments involving algebras of sets.

\begin{prop} \label{prop:prod-s-algebra-generate}
    Suppose each $\A_\alpha$ is generated by $\mathcal{E}_\alpha$. Then $\bigotimes_{\alpha} \A_\alpha$ is generated by the collection \[
        \mathcal{K} = \{\pi_\alpha^{-1} (E_\alpha) : E_\alpha \in \mathcal{E}_\alpha, \alpha \in I\}.
    \]
\end{prop}
\begin{proof}
    Let the collection in \eqref{eq:1d-cylinder-sa} be $\mathcal{J}$. Clearly $\mathcal{K} \subseteq \mathcal{J}$. To see the other inclusion, consider the induced $\sigma$-algebra on $X_\alpha$ \[
        \{E \subseteq X_\alpha : \pi_\alpha^{-1}(E) \in \sigma(\mathcal{K})\},
    \] which contains $\mathcal{E}_\alpha$ and hence $\A_\alpha$. This means $\pi_\alpha^{-1}(E) \in \sigma(\mathcal{K})$ for all $\alpha \in I$ and $E \in \A_\alpha$. Hence $\mathcal{J} \subseteq \sigma(\mathcal{K})$. The proof is now complete.
\end{proof}

We have introduced very general definitions above, but in practice we mostly deal with cases where the index set $I$ is countable. The reader should verify on their own that when $I$ is countable, $\A = \bigotimes_{k=1}^\infty \A_k$ is generated by \[
    \biggl\{\prod_{k=1}^\infty E_k :E_k \in \A_k \biggr\}.
\] Also, for measurable spaces $(X_1,\A_1),(X_2,\A_2),\dotsc$, the product $\sigma$-algebra $\A$ is clearly generated from cylinder sets of the form \[
    I_{n,B} = B \times \prod_{k= n+ 1}^\infty X_n\text{, where }B \in \bigotimes_{k=1}^n \A_k.
\] This turns out to be clean to work with.


3.5.1 3.5.2 Bogachev

Since the Borel $\sigma$-algebra is the $\sigma$-algebra generated by open set, while the topological space consists of all the open sets. With our above detailed comparisons between product $\sigma$-algebras and product topological spaces, the Borel $\sigma$-algebra from the product topology and the product Borel $\sigma$-algebra from individual spaces should be the same, under some conditions.

\begin{thm}
    For any separable metric spaces $X_1,X_2,\dotsc$ (finite or countably infinite), we have \begin{equation} \label{eq:borel-prod-agreement}
        \B(X) = \B(X_1) \otimes \B(X_2) \otimes \dotsb,
    \end{equation} where $X = X_1 \times X_2 \times \dotsb$ with product topology $\mathcal{T}$ given by the supremum metric.
\end{thm}
\begin{proof}
    We follow the proof in \cite{Kallenberg_2002}\footnote{This is the second edition of the book. The new proof in the third edition is very misleading, and I suspect there are many errors in the new edition.}.
    
    Let $\mathcal{J}$ be the class of $1$-dimensional cylinder sets \[
        X_1\times \dotsb \times X_{k-1} \times U_k \times X_{k+1} \times \dotsb
    \] over all $k \in \N$ and $U_k \in \mathcal{T}_k$.
    
    Since $\mathcal{J}$ consists entirely of open sets, and $\text{RHS} = \sigma(\mathcal{J})$ by \cref{prop:prod-s-algebra-generate}, we have $\text{LHS} \supseteq \text{RHS}$. \emph{Note that this inclusion does not use any topological assumptions on the $X_n$'s.}
    
    If we can now show that $\mathcal{T} \subseteq \sigma(\mathcal{J})$, the proof will be complete. Now $(X,\mathcal{T})$, as a product of separable metric spaces, is still a separable metric space. Here we cite a result from \cite{Bogachev_2020}.
    
    \begin{center}
    \noindent\begin{minipage}[t]{0.9\columnwidth}
        \begin{thm*}[{\cite[Theorem 1.2.13]{Bogachev_2020}}]
        Every collection of open sets in a separable metric space contains an at most countable subcollection with the same union.
        \end{thm*}
    \end{minipage}
    \end{center}
    
    Therefore every open set in $X$ is a countable union of basic open sets. Since a topological basis is given by finite intersections of the cylinder sets in $\mathcal{J}$, we then have $\mathcal{T} \subseteq \sigma(\mathcal{J})$.
    % For the countable dense subset $D_k$ of $X_k$, we 
    % Let $\mathcal{T}$ be the product topology on $X_1 \times X_2 \times \dotsb$.Then \[
    %     \text{LHS} = \B(\mathcal{T}).
    % \] Let $\mathcal{C}$ be the collection of $1$-dimensional cylinder sets of the form \[
    %     X_1 \times \dotsb\times X_{k-1}  \times E_k \times X_{k+1} \times \dotsb,
    % \] where $E_k \in \mathcal{T}_k$. Then \[
    %     \text{RHS} = \B(\mathcal{K}).
    % \]
\end{proof}

The direct corollary is that $\B(\R^d) = \bigotimes^d \B(\R^1)$. This theorem overall shows the fundamental importance of Borel $\sigma$-algebra in measure theory and its applications: it connects measurability to the underlying topological spaces.

As an exercise, use \cref{prop:measurability-generate} to show the following: 
\begin{xca}[{\cite[Proposition~2.4]{folland1999}}]
    Given measurable spaces $(X,\M)$ and $(Y_\alpha,\mathcal{N}_\alpha)$ over all $\alpha \in I$. Let $Y = \prod Y_\alpha$ and $\mathcal{N} = \bigotimes \mathcal{N}_\alpha$. Then $f\colon X \to Y$ is $\M/\mathcal{N}$-measurable if and only if each $f_\alpha = \pi_\alpha \circ f$ is $\M/\mathcal{N}_\alpha$-measurable.
\end{xca}


We reserve the discussion of two extremely important existence results about probability measures on product spaces to \cref{sec:product-prob-meas}. The first of the two results () tells us that there is a \emph{natural} extension of product probability measures over all finite cylinder sets to a product probability measure over the entire product $\sigma$-algebra.
The second result () says that if a sequence of probability measures are specified in a \emph{consistent way}, then there is a natural extension of them to a product measure on the entire product $\sigma$-algebra. 

Note that it makes sense to only discuss the countable product of \emph{probability} measures, so that both the coordinate measures, the finite-dimensional product measures. and the countable product measures are all \emph{normalized}. Because of this, and the significance of the existence theorems for product measures in probability, we delay our discussion of these two results despite their purely measure-theoretic statements and proofs.


\section{Integration on product spaces} \label{sec:prod-integrate}
\begin{namedthm}[Fubini--Tonelli theorem]
    
\end{namedthm}

\section{Change of variables} \label{sec:cov}
\section{Gamma functions and polar coordinates} \label{sec:polar}
Let $z \in \C$ with $\Re z > 0$, and we define $f_z\colon (0,\infty)\to \C$ by \[f_z(t) = t^{z-1} e^{-t} = \exp\bigl((z-1)\log t\bigr)\cdot e^{-t}.\] Since 

$\sigma(S^{n-1})=\frac{2\pi^{n/2}}{\Gamma(n/2)}$ and $m(B^{n})=\frac{1}{n}\sigma(S^{n-1})=\frac{\pi^{n/2}}{\Gamma\bigl(\frac{n}{2}+1\bigr)}$.
For any $\epsilon>0$, we have $S^{n-1}\subseteq B^{n}(0;1+\epsilon)-B^{n}(0;1)$
\begin{align*}
m(S^{n-1}) & \leq m\bigl(B^{n}(0;1+\epsilon)\bigr)-m\bigl(B^{n}(0;1)\bigr)\\
 & \leq(1+\epsilon)^{n}m(B^{n})-m(B^{n}).
\end{align*}
Take $\epsilon\to0^{+}$, it is easy to see that $m(S^{n-1})=0$.
surface area

\chapter{Structure of measures}
\section{Hahn--Jordan decomposition of signed measures} \label{sec:signed}
Previously we generalized integrals of nonnegative function to integrals of general signed functions and complex functions. We can make a similar generalization of positive measures to $\R$ and $\C$-valued measures. One of the key goals of this chapter is to explore the intrinsic relationships between measures, functions, and integrals.
\begin{defn}
    Given a measurable space $(X,\A)$, a \df{signed/real measure} (resp.\ \df{complex measure}) on the space is a function $\mu\colon \A \to \R$ (resp.\ $\mu\colon \A \to \C$) such that \begin{enumerate}
        \item \label{enu:emptyset-signed}$\mu(\emptyset) = 0$; 
        \item \label{enu:sigma-additivity-signed-measure} $\mu$ is $\sigma$-additive, i.e., $\mu(E) = \sum_{n=1}^\infty \mu(E_n)$ for all measurable partitions $\{E_n\}$ of $E$.
    \end{enumerate}
\end{defn}

Note condition~\ref{enu:sigma-additivity-signed-measure} implicitly requires the series $\sum \mu(E_n)$ to be absolutely convergent. An important result that says a series is absolutely convergent if and only if any rearrangement of terms in a series yields the same limiting sum; see \cite[Theorems~3.54 and 3.55]{Rudin_principles_1976}. Also note that condition~\ref{enu:sigma-additivity-signed-measure} implies condition~\ref{enu:emptyset-signed}, but we have stated it for clarity.

Many textbooks define the codomain of a signed measure to include one of $+\infty$ or $-\infty$. We do not adopt this convention because it is hardly used in applications, and many complications are avoided. Furthermore, restricting the codomain to the reals allows us to discuss signed and complex measures simultaneously.

In this section, we will state all our proofs for signed measures, which can all be easily extended to complex measures. To distinguish signed/complex measures from the measures we have been discussing previously, we call measures that take nonnegative values \df[positive measure]{positive measures}.

Continuity from above and below still holds for signed and complex measures. The proof here is the same as the one for positive measures.
\begin{xca}
    Let $\mu$ be a signed/complex measure. If $E_n \uparrow E$ or $E_n \downarrow E$ in $\A$, then $\mu(E) = \lim_n(E_n)$.
\end{xca}
Also the inclusion-exclusion formula still holds by $\sigma$-additivity.However monotonicity no longer holds for signed/complex measures, but we may make the following definitions for a signed measure.

\begin{defn}
    For a signed measure $\mu$, a measurable set $A$ is a \df[positive/negative/null set for a signed measure]{positive (negative, or null) set} if for every measurable subset $B$ of $A$, $\mu(B)\geq0$ ($\leq 0$, or $=0$). Equivalently, the measurable set $A$ is positive (negative, or null) if for all $E\in \A$, $\mu(E\cap A) \geq 0$ ($\leq 0$, or $= 0$).
\end{defn}

\begin{namedthm}[Hahn decomposition] \label{thm:hahn-decomp}
    Let $\mu$ be a signed measure on $(X,\A)$. Then $X$ has a partition into $P$ and $N$ such that $P$ is a positive set and $N$ is a negative set.

    Furthermore, if $P'$ and $N'$ is another such partition, then $P \symdiff P' = N \symdiff N'$ is null. This means that the Hahn decomposition is \emph{essentially unique}.
\end{namedthm}
\begin{proof}
    First we show the essential uniqueness. Consider a measurable set $E_1 \subseteq P - P'$. This $E_1$, as a subset of $P$, must have measure $\geq 0$. Yet at the same time $E_1 \subseteq N' - N\subseteq N'$, which implies that $\mu(E_1)\leq 0$. Therefore $\mu(E_1) = 0$. By the same reasoning with $P'$ switching $P$ and $N$ switching $N'$, we should have $\mu(E_2) = 0$ for all measurable subsets $E_2$ of $P'-P$. Since $P\symdiff P' = N \symdiff N' = (P - P')\cup(P' - P)$, it is clear that this is a null set with respect to the signed measure $\mu$.

    Now we prove the existence. We follow the presentation in \cite{Falkner_2019}, which avoids the axiom of dependent choice used in the proofs of most textbook authors.

    To show the existence of the partition $X = P \cup N$, it suffices\footnote{This is also a necessary condition.} to find some measurable $N$ such that for all $E \in \A$, $\mu(E)\geq \mu(N)$. Now we prove this claim. By assumption we have $\mu(N)\leq \mu(\emptyset) = 0$. Now for any $A \in \A$, we have \[
        \mu(N) + \mu(N\cap A) \leq \mu(N-A) + \mu(N\cap A) = \mu(N).
    \] Therefore $N$ is a negative set. For any $A \in \A$, we also have $P\cap A = A - N$ and \[
        \mu(N) \leq \mu(A) = \mu(A - N) + \mu(N).
    \] Therefore $\mu(P\cap A)\geq 0$, which means $P$ is a positive set.
    
    Now we find such an $N$ with the smallest measure over all measurable sets. Let $L = \inf\{\mu(A): A\in \A\}$, then we need to find $N \in \A$ such that $L = \mu(N)$. Since $\A \neq \emptyset$, by countable choice we can take a sequence $\{D_n\}\subseteq \A$ with $\mu(D_n)\to L$.

    Let $\A_n$ be the algebra of subsets of $\cup_{n=1}^\infty D_n$ generated by $\{D_k\}_{k=1}^n$, which is a finite collection\footnote{As an exercise, show that the $\sigma$-algebra generated by a collection of $n$ sets can have at most $2^{2^n}$ sets.}. Therefore $\mu_n \coloneqq \mu|_{\A_n}$ achieves its minimum on the collection $\A_n$, say at $E_n$. Note the same argument that proved the sufficient condition for finding a Hahn decomposition clearly works for the premeasure $\mu|_{\A_n}$ on the algebra $\A_n$: we have $E_n$ is a $\mu_n$-negative set and $E_n^\cpl$ is a $\mu_n$-positive set on $\A_n$.

    We claim that the desired $N = \liminf_m E_m$. First let $A_m^n=\cap_{k=m}^n E_m$ and let $A_m = \cap_{k\geq m}E_m$. Then  \[\mu(A_m^n)\to \mu(A_m)\] as $n\to \infty$. Furthermore the limit above is a decreasing one: note \begin{align*}
        \mu(A_m^{n-1}) & = \mu(A_m^n) + \mu(A_m^{n-1} - E_k) \\
        & = \mu(A_m^n) + \mu(A_m^{n-1}\cap E_n^\cpl) \\
        & \geq \mu(A_m^n), 
    \end{align*}
    where the last inequality follows from the observation that $E_n^\cpl$ is $\mu_n$-positive set on $\A_n$ and $A_m^{n-1} \in \A_n$.

    Now by our choice of $E_m$, we have \[
        \mu(D_m) \geq \mu(E_m) = \mu(A_m^m)\geq \mu(A_m^{m+1}) \geq \dotsb.
    \] Therefore \[\mu(D_m) \geq \mu(A_m) \geq L,\] and taking $m \to \infty$ gives us $\mu(A_m) \to L$ as $m\to \infty$. Now the magic takes place. We know $A_m \uparrow \liminf_m E_m$, and thus $\mu(\liminf E_m) = \lim \mu(A_m)$. The two limits must agree, and hence $L = \mu(\liminf E_m)$. This finishes the proof. 
\end{proof}

In the proof above we have constructed some set that attains $\inf\{\mu(A):A\in \A\}$. This implies the boundedness of $\mu$ from both above and below. (Apply the argument to $-\mu$.)

We define the \df[total variation!measure of a signed/complex measure]{total variation} of the signed/complex measure $\mu$ to be a function $\abs{\mu}\colon \mathcal{A} \to [0,\infty]$ \begin{equation} \label{eq:ttl-var-defn}
    \abs{\mu}(E) = \sup\biggl\{\sum_{n=1}^\infty \abs{\mu(E_n)} : \{E_n\}\text{ is a measurable partition of } E\biggr\},
\end{equation}
the maximized ``variation'' over all partitions of a given set in $\A$.

The definition in \eqref{eq:ttl-var-defn} can be significantly simplified. Because the summands are nonnegative, we can break it into two sums: \begin{align*}
    \sum_{n=1}^\infty \abs{\mu(E_n)} & = \sum_{j:\mu(E_j) \geq 0} \abs{\mu(E_j)} + \sum_{k:\mu(E_k) < 0} \abs{\mu(E_k)} \\
    & = \biggl\vert\sum_{j:\mu(E_j) \geq 0} \mu(E_j)\biggr\vert + \biggl\vert\sum_{k:\mu(E_k)<0} \mu(E_k)\biggr\vert \\ & = 
    \abs{\mu(\widehat{E})} + \abs{\mu(\widetilde{E})},
\end{align*}
where $\widehat{E} = \bigcup\{E_j : \mu(E_j) \geq 0\}$ and $\widetilde{E} = \bigcup\{E_k : \mu(E_k) < 0\}$.
Therefore \begin{equation} \label{eq:ttl-var-equiv}
    \abs{\mu}(E) = \sup\{ \abs{\mu(E_1)} + \abs{\mu(E_2)} : E_1\text{ and } E_2 \text{ are measurable and partition } E\}.
\end{equation} It is clear that we may also take finite partitions here. We may also take the partition to a measurable partition of any measurable subsets of $E$ instead.

By the equivalent definition in \eqref{eq:ttl-var-equiv}, since $\mu$ is a bounded function on $\A$, $\abs{\mu}$ is also bounded. This is in fact the hardest part\footnote{There is a very interesting direct argument that proves the finiteness of $\abs{\mu}$ using the axiom of dependent choice; see \cite{Rudin_realcomplex_1987,Ambrosio_2011,Axler_2020}.} of establishing the following fact.
\begin{thm}
    The total variation $\abs{\mu}$ of a signed/complex measure $\mu$ is a finite positive measure on $(X,\A)$.
\end{thm}
\begin{proof}
    Obviously $\abs{\mu}(\emptyset) = 0$. It remains to check $\sigma$-additivity.
\end{proof}

\begin{defn}
    Let the space of signed (resp.\ complex) measure on $(X,\A)$ be denoted by $\M(X)$. The \df[total variation!norm]{total variation norm} is defined to be the function $\nm{\blank}\colon \M(X)\to \R$ (resp.\ $\C$) given by \[
        \nm{\mu} = \abs{\mu}(X).
    \]
\end{defn}

Let us first show that this $\nm{\blank}$ is indeed a norm on $\M(X)$.

\begin{thm}
    The space of signed/complex measures $\M(X)$ with the total variation norm is a Banach space.
\end{thm}
\begin{proof}
    
\end{proof}

The most important implication of \nameref{thm:hahn-decomp} is a \emph{unique} decomposition of a signed measure $\mu$ into a positive and negative part, known as the \emph{Jordan decomposition}. As we will see soon, the Jordan decomposition offers another characterization of the total variation measure we have just discussed.

Before we start, we need an additional definition.

\begin{defn}
    Let $\mu$ and $\nu$ be two positive/signed/complex measures on $(X, \A)$. We say $\mu$ and $\nu$ are \df{mutually singular}, denoted by $\mu \perp \nu$, if $X$ can be partitioned into two measurable subsets $A$ and $B$, such that \[
        \mu(B) = 0 \quad \text{and} \quad \nu(A) = 0,
    \] or equivalently, for all $E\in \A$, \[
        \mu(E) = \mu(E\cap A) \quad \text{and} \quad \nu(E) = \nu(E \cap B).
    \]
\end{defn}

\begin{namedthm}[Jordan decomposition]
    Let $\mu$ be a signed measure on $(X,\A)$. Then there exist unique two finite positive measures $\mu^+$ and $\mu^-$ on $(X,\A)$ such that \[
        \mu = \mu^+ - \mu^- \quad \text{and} \quad \mu^+ \perp \mu^-.
    \]
\end{namedthm}
\begin{defn} \label{def:abs-cont}
     Let $\mu$ be a positive measure and $\nu$ be a positive/signed/complex measure on $(X, \A)$. We say $\nu$ is \df[absolutely continuous measures]{absolutely continuous} with respect to $\mu$, or $\nu$ is \df[dominating measure]{dominated by} $\mu$, denoted by $\nu \ll \mu$, if for all $E\in \A$, \begin{equation} \label{eq:def-abs-cont}
         \mu(E) = 0 \implies \nu(E) = 0.
     \end{equation}
        
    More generally, to define absolute continuity $\nu \ll \mu$ for signed/complex $\mu$, we change \eqref{eq:def-abs-cont} to \begin{equation} \label{eq:def-abs-cont-sign-cplx}
        \abs{\mu}(E) = 0 \implies \nu(E) = 0.
    \end{equation} This is a definition not used much in practice.
\end{defn}
One should check that $\nu \ll \mu$ if and only if $\abs{\nu} \ll \mu$ if and only if $\nu^+ \ll \mu$ and $\nu^- \ll \mu$. Also check that $\nu$ and $\nu$ are \df{equivalent measures}, in the sense that \[
    \nu \ll \abs{\nu} \ll \nu.
\]


\section{Radon--Nikodym theorem and Lebesgue decomposition}
Depending on what kind of measures we are looking at, there exists multiple versions of the Radon--Nikodym theorem. The following version is the most basic one in practice. It considers a pair of $\sigma$-finite and finite measures.
\begin{namedthm}[Radon--Nikodym theorem] \label{thm:Radon-Nikodym}
    Let $\mu$ be a $\sigma$-finite measure and $\nu$ be a finite measure on $(X,\A)$, where $\nu \ll \mu$. Then there exists an $\A$-measurable function $f$ such that \[
        \nu(E) = \int_E f \,d\mu \quad \text{for all }E\in \mathcal{A}.
    \]
    Furthermore this $f$ is nonnegative and unique in $L^1(X,\A,\mu)$.

    If the $\nu$ above is given as a signed/complex measure instead, then the same conclusions still hold after dropping $f$ is nonnegative. If $\nu$ is given as a $\sigma$-finite measure instead, the function $f$ becomes nonnegative real-valued\footnote{i.e., $f$ takes values in $[0,\infty)$.}, and is unique a.e.
\end{namedthm}
Our $f$ here is called the \df{density/Radon--Nikodym derivative} of $\nu$ with respect to $\mu$, denoted by $d\nu/d\mu$.

We summarize two standard proofs of this theorem. The first of which uses results from Hilbert spaces, while the second one is based on variational principles.
\begin{proof}[Proof 1, using Hilbert spaces]
    
\end{proof}
\begin{proof}[Proof 2, using variational principles]
    
\end{proof}

\begin{namedthm}[Lebesgue decomposition]\label{thm:Leb-decomp}
     Let $\mu$ be a positive measure and $\nu$ be a signed/complex measure on $(X,\A)$. Then 
     \begin{enumerate}
         \item \label{enu:decomp} there exist two unique signed/complex measures $\nu_a$ and $\nu_s$ on $(X,\mathcal{A})$ such that \[
            \nu = \nu_{a} + \nu_{s}\text{, where }\nu_a \ll \mu \text{ and } \nu_s \perp \mu;
         \]
         \item \label{enu:derivative} % there exists an $\A$-measurable function $f$, nonnegative and unique in $L^1(X,\A,\mu)$, such that \[\nu_a(E) = \int_E f \,d\mu \quad \text{for all }E\in \mathcal{A}.\]
     \end{enumerate}
\end{namedthm}

We briefly discuss Lebesgue decomposition for other types of measures below.
\begin{itemize}
    \item If $\nu$ is given as a positive/finite/$\sigma$-finite measure instead, then ``positive'' becomes ``positive''/``finite''/``$\sigma$-finite'' in conclusion~\ref{enu:decomp}.
    \item If $\nu$ is given as a $\sigma$-finite measure instead, then in conclusion~\ref{enu:decomp} $\nu_a$ and $\nu_s$ become $\sigma$-finite.
    \item Conclusion~\ref{enu:decomp} continues to hold if $\mu$ and $\nu$ are both signed or complex. Recall the definition of absolute continuity in this case from \eqref{eq:def-abs-cont-sign-cplx}.
    \item The theorems can be generalized to the case when $\mu$ has no assumption while $\nu$ is an \df[s-finite measure@$s$-finite measure]{$s$-finite measure}, which is a sum of countably many finite measures. See \cite{Falkner_2019}.
\end{itemize}
\begin{rem}
    % This paragraph is borrowed from \cite[Section 3.2]{Bogachev_2007}.
    
    % Given a $\sigma$-finite positive measure $\mu$ on $(X,\A)$, then every finite nonnegative measurable function (not necessarily integrable) defines the $\sigma$-finite positive measure $d\nu_{a} = f d\mu.$ Since $X$ can be written as $\cup_{n=1}^\infty X_n$, where each $X_n$ is $\mu$-finite. 
    If $\nu$ is given as a signed measure instead, then write $\nu = \nu^+ - \nu^-$, and then use the above version of \nameref{thm:Leb-decomp} to write
    
    For each $n\in \N$, set $\nu_n (E) = \nu(E\cap X_n)$ for all $E\in \A$ and get a finite measure $\nu_n$. Now apply \nameref{thm:Leb-decomp} for finite $\nu$ above
\end{rem}

Radon--Nikodym derivative with respect to counting measure
\section{Differentiation}
\section{Functions of bounded variations}
\section{Absolutely continuous functions}
\begin{defn}
    Let $I \subseteq \R$ be an interval. A function $f\colon I\to \R$ is absolutely continuous if for all $\epsilon > 0$, there exists $\delta > 0$ such that \[
        \sum_{i=1}^n (b_i - a_i) < \delta \implies \sum_{i=1}^n \abs{f(b_i) - f(a_I)} <\epsilon
    \] holds for any finite family of pairwise disjoint open intervals $\{(a_i,b_i)\}_{i=1}^n$ contained in $I$.
\end{defn}
\section{Fundamental theorem of calculus}
\begin{namedthm}[Fundamental theorem of calculus (for Lebesgue integrals)]
    For $f\colon [a,b]\to \R$, the following are equivalent: 
    \begin{enumerate}
        \item $f$ is absolutely continuous; 
        \item there exists a Lebesgue integrable function $g$ on $[a,b]$ such that \[
            f(x) = f(a) + \int_a^x g(t)\,dt
        \] for all $x \in [a,b]$.
        \item $f$ has derivative $f'$ almost everywhere, and $f'$ is Lebesgue integrable with \[
            f(x) = f(a) + \int_{a}^x f'(t)\,dt
        \] for all $x \in [a,b]$.
    \end{enumerate}
\end{namedthm}

Bogachev 5.4.5 4.7.60

\chapter{Lebesgue spaces}
    \section{When \texorpdfstring{$1 \leq p < \infty$}{1 <= p < infinity}} \label{sec:Lp-1-infty}
\begin{namedthm}[Hölder's inequality]

\end{namedthm}

\begin{namedthm}[Minkowski's inequality]
    
\end{namedthm}

\begin{thm}
    $L^p$ is complete.
\end{thm}

\section{When \texorpdfstring{$p = \infty$}{p = infty}}
\begin{thm}
    $L^\infty$ is complete.
\end{thm}

\section{The Hilbert space \texorpdfstring{$L^2$}{L2}}

\section{Dual spaces}

\newpage
\phantomsection
\chapter*{\Large Interlude: Between Measure and Probability}
\addcontentsline{toc}{part}{Interlude}
\chaptermark{Interlude}

\newpage

\part{Probability}
\chapter{Interpreting probability using measure theory}
\section{Distributions}
From now on ($\sigma$-)algebras will be called {($\sigma$-)fields}. The measure space $(X,\A,\mu)$ will be replaced by $(\Omega,\F,P)$ with $P(\Omega)=1$, which we call a \df{probability space}. In the probability triplet $\Omega$ is called the \df{sample space}, and $\F$ is called the \df{event space}, which contains all the possible \df[event]{events}. If $\Omega$ is a countable set and $\F = \wp(\Omega)$, then the probability space is \df[discrete probability space]{discrete}. 

Given an underlying measurable spaces $(\Omega,\F)$, a measurable function $X\colon (\Omega,\F) \to (S,\mathcal{S})$ is called a \df{random variable}. If $(\Omega,\F,P)$ is discrete, then the image of any function $X$ is forced to be countable. We may then let $S = X(\Omega)$ and $\mathcal S = \wp(S)$, and $X$ is obviously measurable. The random variable defined on a discrete space is called a \df{discrete random variable}, and its distribution is also \df[discrete distribution]{discrete}. If $(S,\mathcal S)$ is a measurable subspace of $(\R,\B)$, we call the random variable \df[real-valued random variable]{real-valued}. In general when $(S,\mathcal S)$ is a measurable subspace of $(\R^d,\B^d)$, then $X$ is called a \df{real random vector}.

Given a random variable $X$, following \cref{sec:image-measure} we may define a probability measure $\mu$ on $(S,\mathcal{S})$ given by \begin{equation} \label{eq:official-prob-dist-defn}
    \mu(A) = P\bigl(X^{-1}(A)\bigr) = P(X\in A) \text{ for all }A\in \mathcal{S}.
\end{equation}
We call this the \df{probability distribution/law}\footnote{Another common notation is $\mathcal{L}$ that stands for ``law''.} of $X$, denoted by $X \sim \mu$. It characterizes how probability of (the image of) $X$ is distributed across the codomain $(S,\mathcal S)$\footnote{In comparison, $P$ characterizes the \emph{underlying} space $(\Omega,\F)$.}. The $X \in A$ above is a shorthand for $\{\omega\in \Omega:X(\omega)\in A\}$, and this convention\footnote{In fact we have used this shorthand before, when discussing uniform integrability.} is widely adopted throughout probability, as long as the context is clear. It also corresponds to the intuitive understanding of a random variable $X$ as a ``variable'' taking random values by ignoring the underlying $\omega$, but we must not take this formally. When two $\mathcal{F}/\mathcal{S}$-random variables $X$ and $Y$ have the same distribution $\mu$, we write $X =_d Y$.

It is clear that a measure $\mu$ on a measurable subspace of $(\R,\B)$ can be naturally extended to a measure on $(\R,\B)$ (by setting all the new sets to measure $0$). Therefore it always makes sense to regard the distribution of any real-valued random variable as a Borel measure on $\R$.

\begin{rem}
    Another perspective we can take is to always let real-valued random variables take $(S,\mathcal S)$ to be exactly $(\R,\B)$. In this setup $\mu$ will always be a Borel measure. When $X$ is a random variable with $S \coloneqq X(\Omega)\subsetneq \R$, we can always consider the restriction of the distribution $\mu_X$ to $(S,\B|_S)$ to obtain our adopted definition of probability distribution in \eqref{eq:official-prob-dist-defn}. This alternative perspective is suitable for discussing distribution functions, while our previous perspective is suitable for discussing density functions, as we will see.
\end{rem}

\begin{rem}
Throughout the notes, random variables are \emph{almost always} taken to be real-valued\footnote{We have only discussed the integration of real/complex-valued functions. Some generalizations can definitely be made (to for example, Banach-valued functions/random variables), but it is beyond the scope of this survey.}, and the exceptions should be distinguished by the readers on their own.
\end{rem}

The \df[cumulative distribution function@(cumulative) distribution function]{(cumulative) distribution function} of a real-valued random variable $X$ is defined to be a function $F\colon \R \to [0,1]$ given by \[
    F(x) = P(X \leq x) = \mu(-\infty,x].
\]

We now slightly modify \cref{thm:increasing-rcont-Borel-measure-connection}\ref{enu:CDF-measure}\ref{enu:measure-CDF} to suit our purpose. Note now we instead start with the original part~\ref{enu:measure-CDF}.
\begin{thm} \label{thm:measure-CDF-prob}
    Let $X$ be a real-valued random variable with distribution $\mu$ on $(\R,\B)$, then its distribution function $F$ has the following properties:
        \begin{itemize}
            \item $\lim_{x \to -\infty} F(x) = 0$ and $\lim_{x \to \infty} F(x) = 1$;
            \item it is increasing and right-continuous; 
            \item it has left limits in the sense that \[
                F(x-) = \lim_{y \to x^-} F(y) = \mu(-\infty,x),
            \] which also implies $\mu\{x\} = F(x) - F(x-)$.
        \end{itemize}
\end{thm}
Since $\mu$ is now a probability measure, the first bullet point follows directly. The rest has been proved already before.

Recall \cref{thm:increasing-rcont-Borel-measure-connection}\ref{enu:CDF-measure}. We can slightly modify its statement and proof to get the version for obtaining a unique Borel probability measure.

\begin{thm} \label{thm:CDF-measure-prob}
    Conversely, let $F\colon \R\to [0,1]$ be an increasing, right-continuous function with \[
        \lim_{x \to -\infty} F(x) = 0\quad \text{and} \quad \lim_{x \to \infty} F(x) = 1,
    \] then there is a unique probability measure $\mu$ on $(\R,\B)$ such that \[
        \mu(-\infty,x] = F(x) \quad \text{for all }x\in \R.
    \]
\end{thm}

\Cref{thm:CDF-measure-prob} tells us that as long as we have the distribution function of a random variable $X$, which increases from $0$ to $1$ and is right-continuous, then the distribution function determines the distribution of the random variable. Formally we can state 
\begin{cor}
    For two real-valued random variables $X$ and $Y$, we have $F_X = F_Y$ if and only if $\mu_X = \mu_Y$.
\end{cor}

This observation further suggests that given a random variable, we may specify its distribution solely in terms of a function $F\colon \R \to [0,1]$ that is increasing, right-continuous, with \[
        \lim_{x \to -\infty} F(x) = 0\quad \text{and} \quad \lim_{x \to \infty} F(x) = 1.
\] We call such a function $F$ a \df[cumulative distribution function@(cumulative) distribution function]{(cumulative) distribution function} on its own. And we write $X \sim F$ if $X \sim \mu_F$, the unique probability measure associated to the distribution function $F$.

\begin{thm}
    Indeed any distribution function $F \colon \R \to [0,1]$ can be realized as the distribution function of some random variable $X$ on some probability space $(\Omega,\F,P)$.
\end{thm}
\begin{proof}[First construction]
    By \cref{thm:CDF-measure-prob}, we know every distribution function $F$ gives rise to a unique probability measure $\mu$ on $(\R,\B)$. Now let $(\Omega,\F,P) = (\R,\B,\mu)$ and let $X$ be the identity map on $\R$.
\end{proof}
As long as one knows \cref{thm:CDF-measure-prob}, this first proof is indeed a very trivial construction. The second proof, independent of \cref{thm:CDF-measure-prob}, might be a bit more interesting to us now.
\begin{proof}[Second construction]
    Let $(\Omega,\F,P) = ([0,1],\B_{[0,1]},m)$, and we define \[
        X = \sup\{y : F(y) < \omega\} = \inf\{y : F(y) \geq \omega\}
    \]
\end{proof}
We will generalize this result later.

Let $X\colon (\Omega,\F)\to (S,\mathcal S)$ have distribution $\mu$, and the codomain $(S,\mathcal S)$ has a natural underlying measure $\rho$ with $\mu \ll \rho$. The \df[probability density function@(probability) density function]{(probability) density function} (p.d.f.)\ of the random variable $X$ is Radon--Nikodym derivative $d\mu/d\rho$ of the probability distribution with respect to this underlying measure for the image space.

Specifically, when $X$ is a discrete random variable, then the counting measure is a natural measure for $(S,\mathcal S)$, and obviously $\mu \ll \mathrm{count}$. Hence $d\mu/d(\mathrm{count})\colon x \mapsto \mu\{x\}$ is the density function, which is also called the \df{probability mass function} (p.m.f.)\footnote{to emphasize we are in the discrete setting}.

On the other hand, recall \cref{fact:restrict-meausre-restrict-space}. Given a random variable $X$, if the codomain $S$ is a Borel subset of $\R$ and $\mathcal S = \B|_S$, and in addition $\mu \ll m|_S$, then $d\mu/d(m|_S)$ is the density function. We call such $X$ a \df{continuous random variable}. Note in this continuous case the density function is a.e.\ defined, but in the discrete case the density (p.m.f.)\ is exact. Later on when discussing continuous random variables, we usually only write out the case $(S,\mathcal{S}) = (\R,\B)$ for brevity. Since the density function $d\mu / d(m|_S)$ defined on $S$ can naturally be extended to the entire $\R$.

We can define the class of distributions with densities solely in terms of their density functions. When the distribution of $X$ is discrete, it is clear that we can specify the distribution using a \df{probability mass function} (on its own), i.e., a function $p\colon X(\Omega) \to [0,1]$ such that \[\sum_{x\in X(\Omega)} p(x) = 1.\] When the distribution of $X$ is continuous, then a nonnegative Borel-measurable function $f$ satisfying \[
    \int_\R f(x)\,dx = 1,
\] called a \df[probability density function@(probability) density function]{(probability) density function} (on its own) specifies the distribution. In summary, probability mass and density functions let us generate discrete and continuous random variables.

A polish space is a separable topological space that admits a complete metrization.



\section{Independence}
We say a finite collection of events $A_1,\dotsc,A_n \in \F$ are \df{independent} if \[
    P\biggl(\bigcap_{j=1}^n A_j\biggr) = \prod_{j=1}^n P(A_j).
\] An infinite collection of events $A_\alpha$ ($\alpha \in I$) are \emph{independent} if any finite subcollection of the $A_\alpha$'s are independent.

\begin{prop}
    
\end{prop}

\begin{prop}
    
\end{prop}

The \df[tail sigma-field@tail $\sigma$-field]{tail $\sigma$-field} of a sequence of random variables $X_1,X_2,\dotsc$ to be \[
    \mathcal{T} = \bigcap_{n=1}^\infty \sigma(X_n,X_{n+1},\dotsc).
\]

\begin{namedthm}[Kolmogorov's zero--one law]
    Let $X_1,X_2,\dotsc$ be a sequence of independent random variables, then any event in its tail $\sigma$-field has probability $0$ or $1$.
\end{namedthm}

Given $(\Omega,\F,P)$, and let $X$ and $Y$ be two random variables $(\Omega,\F)\to (S,\mathcal S)$ with distributions $\mu_X$ and $\mu_Y$, respectively. The \df{joint distribution} $\mu_{X,Y}$ of $X$ and $Y$ is given by \[
    \mu_{X,Y}(A) = P\times P\bigl((X,Y) \in A\bigr)\quad\text{for all }A\in \mathcal{S}\otimes\mathcal{S}.
\] Our $P\times P$ here is a product probability measure on $(\Omega\times \Omega, \F\otimes \F)$.

Recall that the product measure is the unique extension of the product of marginal measures on measurable rectangles. It is easy to derive using the definition that 
\begin{fact}
    $\mu_{X,Y} = \mu_X \times \mu_Y$ if and only if $X$ and $Y$ are independent.
\end{fact}

The definition of joint distributions can obviously be generalize to any finite number of random variables, by our previous discussions on product measure spaces.

\section{Moments}\label{sec:expec}

The average value of function

Following the theory of Lebesgue integration we have developed, 

\begin{defn}
    Let $X$ be a nonnegative random variable, its \df{expectation/expected value} is given by \[
    \E X = \int_\Omega X\,dP.
    \]
    
    If $X$ is a signed real-valued random variable, with one of $\E X^+$ and $\E X^-$ being finite, then we can define the \emph{expectation} of $X$ to be \[
        \E X = \int_\Omega X\,dP = \E X^+ - \E X^-.
    \]
    In particular, when $\E \abs X < \infty$\footnote{One often prefers to write $\E\abs{X} < \infty$ for integrability of $X$ in probability. However, when we are dealing integration with respect to two different measures, then the $L^1$ notation should again be helpful.}, $\E X$ always exists. This is the case we are interested in mostly.
\end{defn}

Since the distribution $\mu$ on $(S,\mathcal{S})$ is given as the image measure $P\circ X^{-1}$, by \cref{prop:image-meas-cov} we have for $f\colon (S,\mathcal S) \to (\R,\B)$, if $f \geq 0$ or $f\circ X \in L^1(\Omega)$, then \[
    \E f(X) = \int_\Omega f\bigl(X(\omega)\bigr)\,dP(\omega) = \int_S f(x) \,d\mu(x).
\] In particular, if $X$ is real-valued, then \[
    \E X = \int_\Omega X(\omega)\,dP(\omega) = \int_S x\,d\mu(x).
\] Furthermore, if $X$ is discrete, then \[
    \E X = \sum_{x \in S} x \mu\{x\}; 
\] and if $X$ is continuous with density $f$, then \[
    \E X = \int x f(x)\,dx
\]



\begin{namedthm}[Cauchy--Schwarz inequality] \label{thm:c-s-ineq-prob}
    \[\E \abs{XY} \leq \bigl(\E X^2\bigr)^{1/2} \bigl(\E Y^2\bigr)^{1/2}\]
\end{namedthm}

\begin{defn}
    We say two $L^2$ random variables $X$ and $Y$ are uncorrelated if $\E X \cdot \E Y = \E(XY)$.
\end{defn}
The $L^2$ requirement is necessary for us to assert the integrability of $XY$, by \nameref{thm:c-s-ineq-prob}.

\begin{namedthm}[Jensen's inequality] \label{thm:Jensen-prob}
    Let $\E\abs X <\infty$. Suppose $I$ is an interval containing the range of $X$, and we have a convex function $\phi\colon I\to \R$ such that $\phi\circ X\in L^1$. Then \[
        \phi(\E X) \leq \E \phi(X)
    \]
\end{namedthm}
    
\begin{namedthm}[Lyapunov's inequality]
For $p \leq q$, we have 
    \[L^1 \supseteq L^2 \supseteq \dotsb \supseteq L^\infty.\]
\end{namedthm}

\section{Basic concentration and deviation inequalities}
\begin{namedthm}[Generalized Markov's inequality]
    Let $\phi\colon \R \to [0,\infty)$ be increasing. Then for any random variable $X$, and any $a\in \R$ with $\phi(a)\neq 0$, we have \[
        P(X \geq a) \leq \frac{1}{\phi(a)}\E \phi(X).
    \]
\end{namedthm}

\begin{namedthm}[Markov's inequality]
    Let $0< p < \infty$. For any $a > 0$, we have \[
        P(\abs X \geq a) \leq \frac{1}{a^p} \E(\abs X^p).
    \]

    In particular, for nonnegative $X$, we have \[
        P(X \geq a) \leq \frac{\E X}{a}.
    \]
\end{namedthm}

\begin{namedthm}[Chebyshev's inequality]
    For $X$ with $\E X^2 < \infty$, we have for all $t > 0$ that \[
        P(\abs{X - \E X} \geq t) \leq \frac{\Var(X)}{t^2}.
    \]
\end{namedthm}

\begin{namedthm}[Paley--Zygmund inequality] \label{thm:PZ-ineq}
    Let $X \geq 0$ with $\E X^2 < \infty$. For any $0\leq \theta\leq 1$, we have \[
        P(X > \theta \E X) \geq (1-\theta)^2 \frac{(\E X)^2}{\E X^2}.
    \]
\end{namedthm}

\section{Product probability measures} \label{sec:product-prob-meas}
It is noteworthy that both results use the axiom of dependent choice in the proof.
\begin{thm}[(Existence of product measures on countable spaces)]
    The probability premeasure $\mu_0$ defined above is $\sigma$-additive, and hence by \nameref{thm:Caratheodory-ext}, there is a unique extension of $\mu_0$ to a probability measure on $\bigotimes_{n=1}^\infty \F_n$.
\end{thm}

\begin{namedthm}[Daniell--Kolmogorov extension/existence theorem]
    
\end{namedthm}

\section{Misc}
\begin{defn}
    Fix the dimension $d$. The \df{standard Gaussian measure} on $\R^d$ is the measure $\gamma\colon \B(\R^d) \to [0,\infty]$ given by \[
        \gamma(A) = \frac{1}{\bigl(\sqrt{2\pi}\bigr)^n} \int_A \exp\bigl(\nm{x}^2_2/2\bigr)\,dx.
    \]
\end{defn}

It is quite clear that $m$ and $\gamma$ are equivalent measures, since $\exp(\blank)$ is nonnegative.

Bogachev Theorem 1.4.3.

We now restate \nameref{thm:BorelCantelli-meas-th}.

\begin{namedthm}[Borel--Cantelli lemma, part I]
    For events $A_1,A_2,\dotsc$, if $\sum_n P(A_n) < \infty$, then \[
        P(A_n \text{ i.o.}) = 0.
    \]
\end{namedthm}

\begin{namedthm}[Borel--Cantelli lemma, part II]
    For pairwise independent events $A_1,A_2,\dotsc$, if $\sum_n P(A_n) = \infty$, then \[
        P(A_n \text{ i.o.}) = 1.
    \]
\end{namedthm}

The proof is much easier if we assume that the events are independent.

\begin{proof}
    
\end{proof}


\chapter{Modes of convergence in probability}
\section{Statistical distances}
\begin{namedthm*}[Important disclaimer]
    This section deals purely with comparisons of probability measures $\mu$ and $\nu$ on a given measurable space $(S,\mathcal S)$, and has nothing to do with random variables. In practice we may want to see $\mu$ and $\nu$ indeed as probability distributions of random variables on the codomain space $(S,\mathcal S)$. Please be very careful about this distinction.
\end{namedthm*}

Given two probability measure $\mu$ and $\nu$ on $(S,\mathcal S)$, we have the signed measure $\mu - \nu \colon \F \to [-1,1]$. Its total variation norm \begin{align*}
    \nm{\mu - \nu} & = \abs{\mu-\nu}(\Omega) \\
    & = \sup_{A\in \mathcal S}\abs{(\mu-\nu)(A)} + \abs{(\mu-\nu)(\Omega - A)} \\
    & = \sup_{A \in \mathcal S} \abs{\mu(A) - \nu(A)} + \abs{1 - \mu(A) - 1 + \nu(A)} \\
    & = 2\sup_{A\in \mathcal S}\abs{\mu(A) - \nu(A)}.
\end{align*}

The factor $2$ above is usually dropped in probabilistic applications. We define the \df[total variation!distance between probability measures]{total variation distance} between $\mu$ and $\nu$ to be \[
    \tv{\mu-\nu} = \sup_{A \in \mathcal S} \abs{\mu(A) - \nu(A)}.
\] It should be clear that the absolute value sign can be dropped in the definition above, since \[\mu(A) - \nu(A) = \nu(A^\cpl) - \mu(A^\cpl).\]

The following result is a restatement of something we have proved in \cref{sec:signed}.
\begin{fact}
    If $\mu$ and $\nu$ have a common dominating measure $\rho$, then \[
        \tv{\mu - \nu} = \frac{1}{2} \int_{S} \biggl\vert\frac{d\mu}{d\rho}(\omega) - \frac{d\nu}{d\rho}(\omega)\biggr\vert\,d\rho.
    \]
    In particular, if $(S,\mathcal S)$ is a discrete space, then \[\tv{\mu - \nu} = \frac{1}{2}\sum_{x \in S} \abs{\mu\{x\} - \nu\{x\}}.\] And if $(S,\mathcal S) = (\R,\B)$, with $\rho$ being the Lebesgue measure, then \[\frac{1}{2}\int_\R \abs{f(x) - g(x)} \,dx,\] where $f = \frac{d\mu}{d\rho}$ and $g = \frac{d\nu}{d\rho}$ are the two probability densities\footnote{Of course we may consider $\mu$ and $\nu$ on some restricted subspace of $(\R,\B)$, but as mentioned before we drop such consideration for brevity.}. In short, the total variation distance between two probability measures is half the $L^1$ distance between their densities.
\end{fact}
    

The \df{Kullback--Leibler divergence/relative entropy} of $\mu$ with respect to $\nu$ is given by \[
    \KL{\mu}{\nu} = \begin{cases}
        \int_{S} \log\frac{d\mu}{d\nu}\,d\mu & \text{if } \mu \ll \nu,\\
        +\infty & \text{otherwise}.
    \end{cases}
\]
Let $f = \frac{d\mu}{d\nu} \in L^1(\nu)$. It is very important to note that \[\int_S \log\frac{d\mu}{d\nu} \,d\mu = \int_S f\log f\,d\nu\] can be infinite, since $f \log f$ might not be integrable with respect to $\nu$. Sometimes we just 

\begin{fact} \label{fact:KL-practice}
    If $\mu \ll \nu\ll \rho$, then \[
        \KL{\mu}{\nu} = \int_{S} \biggl(\frac{d\mu}{d\rho}\biggr) \log\biggl(\frac{d\mu/d\rho}{d\nu/d\rho}\biggr) \,d\rho.
    \]
    Therefore if the space is discrete, then we take $\rho$ to be the counting measure and get \[
        \KL{\mu}{\nu} = \sum_{x\in S} \mu\{x\} \log\frac{\mu\{x\}}{\nu\{x\}}.
    \]
    And if $(S,\mathcal S) = (\R,\B)$, with $\rho$ being the Lebesgue measure, then \[
        \KL{\mu}{\nu} = \int_{\R} f(x) \log\frac{f(x)}{g(x)}\,dx,
    \] where $f = \frac{d\mu}{d\rho}$ and $g = \frac{d\nu}{d\rho}$ are the two probability densities. In this latter case we might as well write $\KL{f}{g}$.
\end{fact}

Given a probability measure $\nu$, for a nonnegative $f \in L^1(\nu)$ such that $f \log f$ is also $\nu$-integrable, we define its \df{entropy functional} to be \[
    \Ent_\nu f = \E_\nu (f\log f) - (\E_\nu f)(\log \E_\nu f),
\] which should be compared with the variance functional \[
    \Var_\nu f = \E_\nu f^2 - (\E_\nu f)^2.
\] But keep in mind the entropy functional can only be applied to ($\nu$-a.e.)\ nonnegative\footnote{If $f = 0$ $\nu$-a.e., since $0\log 0$ is taken to be $0$, we would have no problem.} functions because of the logarithm in the definition. Also note that the entropy functional is homogeneous: we have \[
    \Ent cf = c\Ent f \quad \text{for } c\geq 0,
\] which is ``better'' than \[
    \Var cf = c^2 \Var f \quad \text{for }c\in \R
\] in some applications.

If we have another probability measure $\mu$ with $\mu \ll \nu$, then \[
    \Ent_\nu \frac{d\mu}{d\nu} = \KL{\mu}{\nu}.
\] If $d\mu/d\nu$ can be explicitly expressed by some function $h$ (as discussed in \cref{fact:KL-practice}), then the equation above gives a simple expression for the KL divergence.

Fisher information 5.1.2 Markov diffusion operators LSI

\begin{namedthm}[Pinsker's inequality]
    $\tv{\mu - \nu} \leq \sqrt{\frac 1 2 \KL{\mu}{\nu}}$.
\end{namedthm}

Hellinger distance
Wasserstein

$\phi$-entropy

\section{Laws of large numbers}
\begin{namedthm}[$L^2$ weak law]
    Let $X_1,X_2,\dotsc$ be uncorrelated with equal mean $\mu$ and $\sup_j\Var(X_j) < \infty$. Then \[
        \frac{X_1 + \dotsb + X_n}{n} \to \mu 
    \] in $L^2$ (and hence in probability).
\end{namedthm}

\begin{namedthm}[$L^1$ weak law]
    Let $X_1,X_2,\dotsc$ be i.i.d.\ and integrable, with mean $\mu$. Then \[
        \frac{X_1 + \dotsc + X_n}{n} \to \mu
    \] in probability.
\end{namedthm}

\begin{namedthm}[$L^1$ strong law]
    Let $X_1$
\end{namedthm}

\begin{namedthm}[$L^4$ strong law]
    
\end{namedthm}

\begin{namedthm}[$L^2$ strong law]
    
\end{namedthm}

\begin{namedthm}[Glivenko--Cantelli theorem]
    
\end{namedthm}

\section{Weak convergence on metric spaces}
We use $\M_1(\Omega)$ for the space of Borel probability measures.
\begin{defn}
    
\end{defn}

\begin{namedthm}[Slutsky's theorem]
    Given $X_n \wkconv X$ and $Y_n \wkconv c$, then \begin{enumerate}
        \item $X_n + Y_n \wkconv X + c$;
        \item $X_nY_n\wkconv cX$.
    \end{enumerate}
\end{namedthm}

\begin{namedthm}[Portmanteau Theorem]
    
\end{namedthm}

Helly selection (sequential Banach--Alaoglu) vague convergence Prokhorov's theorem 

The usual proof of the 

\begin{thm}
    Let $\{F_n\}$ be a sequence of distribution functions, then there is a subsequence $\{F_{n_k}\}$ and a right-continuous increasing function $F$ such that \[
        \lim_{k\to \infty} F_{n_k}(x) = F(x)
    \] for all continuity points $x$ of $F$.
\end{thm}

\begin{proof}
    Let $q_1,q_2,\dotsc$ be an enumeration of $\Q$. First $\{F_n(q_1)\}$ is a sequence in $[0,1]$, a bounded interval, and therefore there is a subsequence that converges to $s_1 \coloneqq \liminf_n F_n(q_1)$. We can construct such a subsequence by defining $F_{\nu(n)}(q_1)$ inductively for all $n$: \[
        \nu(n) = \min\{m > \nu(n-1): \abs{F_m(q_1) - s_1} < 1/ n\}.
    \] Let $\{F^1_n\}$ be the new sequence $\{F_{\nu(n)}\}$, and in the same way one can construct its subsequence $\{F^2_n\}$ satisfying $\lim_n F^2_n(q_2) = s_2 \coloneqq \liminf_n F_n^1(q_2)$.
    
    Proceeding in this fashion, one gets a limiting subsequence $F_{n_k}$ such that $\lim_k F_{n_k}(q)$ exists for all $q\in \Q$. Let $G\colon \R \to [0,1]$ be the function such that \[
        G(q) = \lim_k F_{n_k}(q).
    \] Take its right-continuous inverse $F$ given by \[
        F(x) = \inf\{G(q): q\in \Q, q > x\}.
    \] Also \[
        G(\tilde q)\leq F(x) \leq G(\hat q).
    \]
\end{proof}

Arzela Ascoli


\begin{namedthm}[Central limit theorem]
    
\end{namedthm}

\chapter{Conditional expectations and martingales}
\begin{defn}
    Let $\E \abs{X} < \infty$, and $\G$ be a sub-$\sigma$-field of $\F$. Define the \df{conditional expectation} of $X$ given $\G$ to be the random variable $Y$ satisfying \begin{enumerate}
        \item $Y$ is $\G$-measurable; 
        \item \label{enu:equal-int-cond-expec} $\E(Y \ind_{G}) = \E(X \ind_G)$ for all $G \in \G$.
    \end{enumerate}
    This $Y$ is denoted by $\E(X \giv \G)$.
\end{defn}

We first show that the above definition makes sense from a purely measure-theoretic point of view, and is unique a.s.
Notice that the function $\nu\colon \mathcal{G} \to \R$ given by \begin{equation} \label{eq:cond-expec-signed-meas}
    \nu(G) = \E(X\ind_G) = \int_G X \,dP
\end{equation} is a signed measure, and $\nu \ll P|_\G$. Therefore by the \nameref{thm:Radon-Nikodym} for a signed measure and a finite positive measure, there exists a random variable $Y$, unique in $L^1(\Omega,\G,P|_\G)$, such that \[
    \nu(G) = \int_G Y \,dP = \E(Y\ind_G)
\] for all $G\in \G$. \emph{Be aware that conditional expectations are unique up to measure zero.}

% Note that there does not exist a version of conditional expectation $\E(X\giv \G)$ defined specifically for nonnegative $X$. This is because the $\nu$ we defined in \eqref{eq:cond-expec-signed-meas} may become possibly infinite. We have discussed previously that there does not exist a nice Radon--Nikodym theorem for a general infinite measure $\nu$.

\begin{defn}
    Define the \df{conditional probability} of $A \in \F$ given a sub-$\sigma$-field $\G$ of $\F$ to be $\E(\ind_A \giv \G)$, which we denote by $P(A\giv \G)$.
\end{defn}

% When $X = \ind_A$, the $\nu(G)$ in \eqref{eq:cond-expec-signed-meas} now becomes $P(A\cap G)$. In a first course in probability we define \[
%     P(A \giv G) = \frac{P(A \cap G)}{P(G)}.
% \] % The conditional probability $P(A \giv \G)$ is therefore the Radon--Nikodym derivative of the probability measure $\nu$ (that takes the information $\G$) with respect to the original probability measure $P$.

Our new definitions of conditional expectation and conditional probability are very abstract, and particularly distinct from the undergraduate version, and the following example is almost included in all textbooks, which explains how our new definitions generalizes the old definitions.

\begin{exa}
    Let $\Omega_1,\Omega_2,\dotsc$ be a countable partition of the sample space $\Omega$, where each $\Omega_n$ has strictly positive measure. In an undergraduate class we would define \[
        \E(X\giv \Omega_n) = \frac{\E(X;\Omega_n)}{P(\Omega_n)}
    \] for any $n$. Now define $\G = \sigma(\{\Omega_n\}_{n=1}^\infty)$. It is easy to see that \begin{equation} \label{eq:agree-two-cond-expec}
        \int_{\Omega_n} \frac{\E(X;\Omega_n)}{P(\Omega_n)} \,dP = \int_{\Omega_n} X\,dP.
    \end{equation} We claim that $\E(X\giv \G)$ is given by \[
        Y = \frac{\E(X;\Omega_n)}{P(\Omega_n)} \text{ on each }\Omega_n,
    \] and hence coincides with our undergraduate definition.

    First the candidate $Y$ is $\G$-measurable since it is a constant on each $\Omega_n$. Also since $\{\Omega_n\}$ is a partition of $\Omega$ and generates $\G$, equation~\eqref{eq:agree-two-cond-expec} immediately implies that \[
        \int_G Y\,dP = \int_G X\,dP
    \] for all $G\in \mathcal{G}$. This finishes the proof.

    Now we look at condition probability. Set $X = \ind_A$, and we have \begin{align*}
        P(A \giv \G) & = \E(\ind_A\giv\G) \\ & = \frac{\E(\ind_A\ind_{\Omega_n})}{P(\Omega_n)} \text{ on each }\Omega_n \\ & = \frac{P(A\cap \Omega_n)}{P(\Omega_n)} \text{ on each }\Omega_n,
    \end{align*} which was our undergraduate definition of conditional probability $P(A\giv \Omega_n)$.
\end{exa}

% The Radon--Nikodym derivative should match on partitions

\chapter{Introductory ergodic theory}
Given a probability space $(\Omega,\F,\mu)$, a \df{measure-preserving transformation} (MPT) $T$ is a measurable function from $(\Omega,\F)$ to itself such that \[
    \mu(T^{-1}A) = \mu(A)\text{ for all }A\in \F.
\] The resulting quartet $(\Omega,\F,\mu,T)$ is called a \df{measure-preserving dynamical system} (MPDS). If $T$ is invertible, and $T^{-1}$ is measurable, then it is equivalent to say $T$ is measure-preserving if \[
    \mu(TA) = \mu(A) \text{ for all }A\in \F.
\]

An MPT $T$ is said to be \df{ergodic} if for all $A\in \F$, we have \[
    \mu(A \symdiff T^{-1} A) = 0 \implies \mu(A) = 0 \text{ or } 1.
\] A set $A\in \F$ satisfying $\mu(A \symdiff T^{-1} A) = 0$ is called \df{(almost) invariant}. If instead we have $T^{-1}A = A$, then $T$ is \df{strictly invariant}. The ergodicity of $T$ can be equivalently defined by \[
    T^{-1} A = A \implies \mu(A) = 0 \text{ or } 1,
\] that is, we only need to check strictly invariant sets must be of measure $0$ or $1$.

One direction is obvious. For the other direction, one can check that for any set $A\in \F$, the set $B = \limsup_n T^{-n} A$ is always going to be strictly invariant.

\begin{defn}
    An MPDS $(\Omega,\F,\mu,T)$ is said to be \df[mixing!strongly]{strong mixing} if \[
        \lim_n \mu(A\cap T^{-n} B) = \mu(A) \mu(B).
    \] It is said to be \df[mixing!weakly]{weak mixing} if \[
        \lim_n \frac{1}{n} \sum_{k=0}^{n-1} \bigl\lvert \mu(A \cap T^{-k} B) - \mu(A) \mu(B) \bigr\rvert = 0,
    \] i.e., $\bigl\lvert \mu(A \cap T^{-k} B) - \mu(A) \mu(B) \bigr\rvert$ converges to $0$ in the Cesàro sense.
\end{defn}

Hence strong mixing implies weak mixing. Most ergodic dynamical systems of interest to probabilists turns out to be strong mixing.

Dyadic transformation



\chapter{Introductory optimal transport}

\chapter{Markov processes}

\chapter{Brownian motions}

\newpage
\phantomsection
\chapter*{\Large Epilogue}
\addcontentsline{toc}{part}{Epilogue}
\chaptermark{Epilogue}
Riesz theorem finite Radon measure (Bogachev 7.1) vague convergence


\appendix
\chapter*{\Large Appendices}
\addcontentsline{toc}{chapter}{Appendices}
\chaptermark{Appendices}
\numberwithin{equation}{section}
\renewcommand\thesection{\@Alph\c@section}

\section{Weak and weak-star topologies on normed spaces}
\begin{namedthm}[Sequential Banach--Alaoglu theorem]
    For a separable normed vector space $X$, every bounded sequence in $X^*$ has a weak-star convergent subsequence (i.e., $X^*$ is weak-star sequentially compact).
\end{namedthm}

\begin{namedthm}[Banach--Alaoglu theorem]
    For a normed vector space $X$, every closed and bounded subset of $X^*$ is weak-star compact.
\end{namedthm}

metrizability

\begin{namedthm}[Tychonoff's theorem]
    Arbitrary product of compact topological spaces is compact.
\end{namedthm}

\begin{thm}[(Tychonoff's theorem for countable product)]
    Countable product of compact topological spaces is compact.
\end{thm}

If the product is finite, then no choice is needed.

\begin{thm}
    The countable product of sequentially compact spaces is sequentially compact.
\end{thm}

\section{Riesz' theorems}
\begin{namedthm}[Riesz' theorem for compact metric spaces]
    Let $(X,d)$ be a compact metric space, then the dual space $C(X)^*$ is isometrically isomorphic to $\M(X)$, i.e., for all linear functionals $L \in C(X)^*$, there is a unique $\mu \in \M(X)$ such that \[
        L(f) = \int_X f\,d\mu\quad \text{for all }f\in C(X);
    \] meanwhile $\nm{L}_{u} = \nm{\mu}$.
\end{namedthm}

\begin{namedthm}[Riesz' theorem for LCH spaces]
    Let $X$ be an LCH space, then the dual space $C_0(X)^*$ is isometrically isomorphic to $\M_r(X)$, i.e., for all linear functionals $L \in C_0(X)^*$, there is a unique $\mu \in \M_r(X)$ such that \[
        L(f) = \int_X f\,d\mu\quad \text{for all }f\in C_0(X);
    \] meanwhile $\nm{L}_{u} = \nm{\mu}$.
\end{namedthm}

\section{Topological groups and Haar measures}

\newpage
% \nocite{*}
\phantomsection
\printbibliography
\addcontentsline{toc}{chapter}{Bibliography}

\newpage
\phantomsection
\printindex
\addcontentsline{toc}{chapter}{List of Definitions}
\end{document}